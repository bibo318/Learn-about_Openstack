# Cài đặt OpenStack TRAIN 

## Cài đặt 3 Controller HA 

### Chuẩn bị 

* Cài đặt 3 máy với các thông tin cấu hình như sau:

	* Controller 1:  

		* Manager + API: eth0 - 192.168.68.81
		* Provider: eth1 - 192.168.70.81
		* DataVM: eth2 - 192.168.50.81

	* Controller 2:  

		* Manager + API: eth0 - 192.168.68.82
		* Provider: eth1 - 192.168.70.82
		* DataVM: eth2 - 192.168.50.82

	* Controller 3:  

		* Manager + API: eth0 - 192.168.68.83
		* Provider: eth1 - 192.168.70.83
		* DataVM: eth2 - 192.168.50.83

* VIP cho Controller: 192.168.68.84

* Set hostname và timezone

```sh
hostnamectl set-hostname controller3
HOSTNAME=controller3
timedatectl set-timezone Asia/Ho_Chi_Minh
```

* Cấu hình file host như sau:

```sh
cat <<EOF >> /etc/hosts

192.168.68.81 controller1
192.168.68.82 controller2
192.168.68.83 controller3
192.168.68.84 controller # VIP

192.168.68.85 compute1
192.168.68.86 compute2
192.168.68.87 cinder1
EOF
```

* Cấu hình selinux

```sh
sed -i s/^SELINUX=.*$/SELINUX=disabled/ /etc/selinux/config
setenforce 0
```

* Cấu hình IP trên các node tương tự trên controller 1 như sau:

```sh
cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth0
# Created by cloud-init on instance boot automatically, do not edit.
#
BOOTPROTO=none
DEVICE=eth0
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
IPADDR=192.168.68.83
PREFIX=24
GATEWAY=192.168.68.1
DNS1=8.8.8.8
EOF

cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-eth1
BOOTPROTO=none
DEVICE=eth1
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
IPADDR=192.168.70.83
PREFIX=24
EOF

cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-eth2
BOOTPROTO=none
DEVICE=eth2
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
IPADDR=192.168.50.83
PREFIX=24
EOF
```

* Khởi động lại network 

```sh
systemctl restart network
```

* Cấu hình chrony trên các node controller

```sh
yum install chrony -y
sed -i "s/server 0.centos.pool.ntp.org iburst/#server 0.centos.pool.ntp.org iburst/g" /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed  -i '1i server vn.pool.ntp.org iburst' /etc/chrony.conf
sed  -i '2i allow 192.168.68.0\/24' /etc/chrony.conf

systemctl enable chronyd.service
systemctl start chronyd.service
chronyc sources
```

* Các node khác:

```sh
yum install chrony -y
sed -i "s/server 0.centos.pool.ntp.org iburst/#server 0.centos.pool.ntp.org iburst/g" /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed  -i '1i server controller iburst' /etc/chrony.conf

systemctl enable chronyd.service
systemctl start chronyd.service
```

* Kiểm tra xem đã đồng bộ được thời gian giữa các node chưa

```sh
firewall-cmd --add-service=ntp --permanent
firewall-cmd --reload
chronyc sources
```

### Cài đặt ban đầu

* Cài đặt Repository của Openstack Train

```sh
yum -y install centos-release-openstack-train
```

### Cài đặt mariadb cluster giữa các node

* Trên cả ba node controller (chú ý một vài chỗ cần đổi ip và tên node trên từng node)

```sh
yum --enablerepo=centos-openstack-train -y install mariadb-server

cat <<EOF > /etc/my.cnf.d/openstack.cnf
[mysqld]

# ip controller
bind-address = 192.168.68.83
default-storage-engine = innodb
innodb_file_per_table
max_connections = 1024
collation-server = utf8_general_ci
character-set-server = utf8
EOF


yum --enablerepo=centos-openstack-train -y install galera rsync mariadb-server-galera.x86_64

cat <<EOF > /etc/my.cnf.d/galera.cnf
[galera]
# Mandatory settings
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so

#add your node ips here
wsrep_cluster_address="gcomm://192.168.68.81,192.168.68.82,192.168.68.83"
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
#Cluster name

wsrep_cluster_name="galera_cluster"
# Allow server to accept connections on all interfaces.

bind-address=192.168.68.83

# this server ip, change for each server
wsrep_node_address="192.168.68.83"

# this server name, change for each server
wsrep_node_name="node3"

wsrep_sst_method=rsync
EOF

# Tạo mật khẩu trên một node controller1
mysql_secure_installation
```

* Nhập mật khẩu cho user root sau đó tiếp tục cấu hình Galera như sau:

```sh
firewall-cmd --add-service=mysql --permanent
firewall-cmd --add-port={3306/tcp,4567/tcp,4568/tcp,4444/tcp} --permanent
firewall-cmd --reload

# Thực hiện trên controller 1 (trước khi chạy lệnh này phải stop mariadb service)
galera_new_cluster
cat /var/lib/mysql/grastate.dat

# Khởi động lại service trên tất cả các node controller theo thứ tự lần lượt, controller1 -> controller 2 -> controller3
systemctl restart mariadb
mysql -u root -ptrang1234 -e "SHOW STATUS LIKE 'wsrep_cluster_size'"
```

* Cấu hình cluster check trên tất cả các node controller

```sh
yum install -y xinetd wget
rm -f clustercheck ; rm -f  /usr/bin/clustercheck
wget https://raw.githubusercontent.com/hungnt1/percona-clustercheck/master/clustercheck
mv  clustercheck /usr/bin && chmod 555 /usr/bin/clustercheck

mysql -u root -ptrang1234 -e "GRANT PROCESS ON *.* TO 'clustercheckuser'@'localhost' IDENTIFIED BY '123@123Aa'"

cat <<EOF>  /etc/xinetd.d/mysqlchk
# default: on
# description: mysqlchk
service mysqlchk
{
        disable = no
        flags = REUSE
        socket_type = stream
        port = 9200
        wait = no
        user = nobody
        server = /usr/bin/clustercheck
        log_on_failure += USERID
        only_from = 0.0.0.0/0
        per_source = UNLIMITED
}
EOF
echo 'mysqlchk 9200/tcp # MySQL check' >> /etc/services
systemctl start xinetd
systemctl enable xinetd

firewall-cmd --add-port={9200/tcp,4444/tcp} --permanent
firewall-cmd --reload
```

### Cài đặt rabbitmq và memcached trên tất cả controller

Thực hiện cài đặt và cấu hình theo các bước dưới đây

* Cài đặt trên cả ba node (**Chú ý**: đổi ip tương ứng với từng node)

```sh
yum --enablerepo=epel -y install rabbitmq-server memcached
systemctl start rabbitmq-server memcached 
systemctl enable rabbitmq-server memcached

rabbitmqctl add_user openstack trang1234
rabbitmqctl set_permissions openstack ".*" ".*" ".*"

cp /etc/sysconfig/memcached /etc/sysconfig/memcached.origin
sed -i 's/OPTIONS=\"-l 127.0.0.1,::1\"/OPTIONS=\"-l 192.168.68.83\"/g' /etc/sysconfig/memcached
systemctl restart rabbitmq-server memcached
```

* Enable plugin và copy erlang cookie từ controller1 sang các node còn lại (chú ý: cần thực hiện lần lượt)

```sh
rabbitmq-plugins enable rabbitmq_management  
rabbitmq-server -detached  

scp  /var/lib/rabbitmq/.erlang.cookie root@controller2:/var/lib/rabbitmq/
scp  /var/lib/rabbitmq/.erlang.cookie root@controller3:/var/lib/rabbitmq/
```

* Join các node còn lại vào cluster (thực hiện trên các node controller còn lại)

```sh
sudo systemctl restart rabbitmq-server
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@controller1
rabbitmqctl start_app
rabbitmqctl cluster_status
```

* Cấu hình HA policy trên 3 node controller

```sh
rabbitmqctl set_policy ha-all '^(?!amq\.).*' '{"ha-mode": "all"}' 
```

* Kiểm tra lại rabbitmq cluster trên controller1 như sau:

```sh
[root@controller1 ~]# rabbitmqctl cluster_status
Cluster status of node rabbit@controller1
[{nodes,[{disc,[rabbit@controller1,rabbit@controller2,rabbit@controller3]}]},
 {running_nodes,[rabbit@controller3,rabbit@controller2,rabbit@controller1]},
 {cluster_name,<<"rabbit@controller1">>},
 {partitions,[]},
 {alarms,[{rabbit@controller3,[]},
          {rabbit@controller2,[]},
          {rabbit@controller1,[]}]}]
```

* Cấu hình firewall

```sh
firewall-cmd --add-port=11211/tcp --permanent
firewall-cmd --add-port=15672/tcp --permanent ## Web Interface
firewall-cmd --add-port=5672/tcp --permanent ## RabitMQ Server
firewall-cmd --add-port={4369/tcp,25672/tcp} --permanent ## RabitMQ Cluster Port
firewall-cmd --reload
```

### Cài đặt keystone

* Tạo database
```sh
mysql -u root -ptrang1234
create database keystone;
grant all privileges on keystone.* to keystone@'localhost' identified by 'trang1234';
grant all privileges on keystone.* to keystone@'%' identified by 'trang1234';
flush privileges;
exit
```

* Install Keystone

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi

cp /etc/keystone/keystone.conf /etc/keystone/keystone.conf.org
cat <<EOF > /etc/keystone/keystone.conf
[cache]
memcache_servers = controller1:11211,controller2:11211,controller3:11211

[cors]
#allowed_origin = http://grafana-server:3000
#allow_methods = GET,PUT,POST,DELETE,PATCH
#allow_headers = X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-Domain-Id,X-Domain-Name

[database]
connection = mysql+pymysql://keystone:trang1234@controller/keystone

[token]
provider = fernet
EOF


cp -np /etc/httpd/conf.d/wsgi-keystone.conf /etc/httpd/conf.d/wsgi-keystone.conf.origin
cp -np /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.origin
echo "ServerName `hostname`" >> /etc/httpd/conf/httpd.conf
sed -i -e "s/^Listen.*/Listen `hostname -i`:80/g" /etc/httpd/conf/httpd.conf
systemctl enable httpd.service
systemctl start httpd.service
systemctl restart httpd.service


# chỉ cần đồng bộ trên một node
su -s /bin/bash keystone -c "keystone-manage db_sync"
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
chown -R keystone:keystone /etc/keystone

cat<<EOF > /etc/httpd/conf.d/wsgi-keystone.conf
Listen `hostname -i`:5000

<VirtualHost `hostname -i`:5000>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    LimitRequestBody 114688
    <IfVersion >= 2.4>
      ErrorLogFormat "%{cu}t %M"
    </IfVersion>
    ErrorLog /var/log/httpd/keystone.log
    CustomLog /var/log/httpd/keystone_access.log combined

    <Directory /usr/bin>
        <IfVersion >= 2.4>
            Require all granted
        </IfVersion>
        <IfVersion < 2.4>
            Order allow,deny
            Allow from all
        </IfVersion>
    </Directory>
</VirtualHost>

Alias /identity /usr/bin/keystone-wsgi-public
<Location /identity>
    SetHandler wsgi-script
    Options +ExecCGI

    WSGIProcessGroup keystone-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
</Location>
EOF




# Khởi tạo fernet repository trên controller1
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone

# Sao chép sang các controller khác (thực hiện lần lượt với từng server)
scp -rp /etc/keystone/credential-keys root@controller3:/etc/keystone/
scp -rp /etc/keystone/fernet-keys root@controller3:/etc/keystone/
ssh root@controller3 "chown -R keystone:keystone /etc/keystone/fernet-keys/"
ssh root@controller3 "chown -R keystone:keystone /etc/keystone/credential-keys/"

```

* Hoàn tất cài đặt keystone thực hiện trên cả ba controller

```sh
cat <<EOF > ~/keystonerc
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=trang1234
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export PS1='[\u@\h \W(openstack)]\$ '
export OS_AUTH_TYPE=password
EOF

chmod 600 ~/keystonerc 
source ~/keystonerc 
echo "source ~/keystonerc " >> ~/.bash_profile
```


### Cài đặt HAproxy + Pacemaker cho 3 node controller

```sh
yum install pacemaker pcs resource-agents -y 
systemctl start pcsd.service
systemctl enable pcsd.service

echo "hacluster:trang1234" | chpasswd

# Khởi tạo cluster trên một node controller1
pcs cluster auth controller1 controller2 controller3 -u hacluster -p trang1234 --force
pcs cluster setup --force --name hacluster controller1 controller2 controller3

pcs cluster start --all
pcs cluster enable --all

corosync-cfgtool -s
pcs status corosync

# Cấu hình Haproxy trên cả 3 node
yum install haproxy -y
cp -np /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.org

## Chú ý IP
cat <<EOF> /etc/haproxy/haproxy.cfg
global
    chroot  /var/lib/haproxy
    daemon
    group  haproxy
    maxconn  4000
    pidfile  /var/run/haproxy.pid
    user  haproxy
 
defaults
    log  global
    maxconn  4096
    option  redispatch
    retries  3
    mode    tcp
    timeout  http-request 60s
    timeout  queue 1m
    timeout  connect 60s
    timeout  client 1m
    timeout  server 1m
    timeout  check 60s
  
listen stats 
    bind 192.168.68.84:9000
    mode http
    stats enable
    stats uri /stats
    stats show-legends
    stats realm HAProxy\ Statistics
    stats auth admin:trang1234
    stats admin if TRUE
 
listen dashboard_cluster
    bind 192.168.68.84:80
    balance  roundrobin
    mode http
    option  httpchk
    option  tcplog
    cookie  SERVERID insert indirect nocache
    server controller1 192.168.68.81:80 cookie controller1 check inter 2000 rise 2 fall 5 
    server controller2 192.168.68.82:80 cookie controller2 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:80 cookie controller3 check inter 2000 rise 2 fall 5 

listen mariadb_cluster 
    bind 192.168.68.84:3306
    mode tcp
    balance roundrobin
    option httpchk GET /
    option tcpka
    option httpchk
    timeout client  28800s
    timeout server  28800s  
    stick-table type ip size 1000
    stick on dst
    server controller1 192.168.68.81:3306 check port 9200 inter 5s fastinter 2s rise 3 fall 3
    server controller2 192.168.68.82:3306 check port 9200 inter 5s fastinter 2s rise 3 fall 3 
    server controller3 192.168.68.83:3306 check port 9200 inter 5s fastinter 2s rise 3 fall 3 
 
listen keystone_public_internal_cluster
    bind 192.168.68.84:5000
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.68.81:5000 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:5000 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:5000 check inter 2000 rise 2 fall 5 
 
listen glance_api_cluster
    bind 192.168.68.84:9292
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.68.81:9292 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:9292 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:9292 check inter 2000 rise 2 fall 5 
 
listen glance_registry_cluster
    bind 192.168.68.84:9191
    balance  roundrobin
    option  tcpka
    option  tcplog
    server controller1 192.168.68.81:9191 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:9191 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:9191 check inter 2000 rise 2 fall 5 
 
listen nova_compute_api_cluster
    bind 192.168.68.84:8774
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.68.81:8774 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:8774 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:8774 check inter 2000 rise 2 fall 5 
 
listen nova_vncproxy_cluster
    bind 192.168.68.84:6080
    balance  roundrobin
    option  tcpka
    option  tcplog
    capture request header X-Auth-Project-Id len 50
    capture request header User-Agent len 50
    server controller1 192.168.68.81:6080 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:6080 check inter 2000 rise 2 fall 5
    server controller3 192.168.68.83:6080 check inter 2000 rise 2 fall 5
 
listen nova_metadata_api_cluster
  bind  192.168.68.84:8775
  balance  roundrobin
  option  tcpka
  option  tcplog
  server controller1 192.168.68.81:8775 check inter 2000 rise 2 fall 5
  server controller2 192.168.68.82:8775 check inter 2000 rise 2 fall 5
  server controller3 192.168.68.83:8775 check inter 2000 rise 2 fall 5

listen nova_placement_api
    bind 192.168.68.84:8778
    balance roundrobin
    option tcpka
    option tcplog
    http-request del-header X-Forwarded-Proto
    server controller1 192.168.68.81:8778 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:8778 check inter 2000 rise 2 fall 5
    server controller3 192.168.68.83:8778 check inter 2000 rise 2 fall 5    

listen neutron_api_cluster
    bind 192.168.68.84:9696
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.68.81:9696 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:9696 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:9696 check inter 2000 rise 2 fall 5 

listen cinder_api_cluster
    bind 192.168.68.84:8776
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.68.81:8776 check inter 2000 rise 2 fall 5
    server controller2 192.168.68.82:8776 check inter 2000 rise 2 fall 5 
    server controller3 192.168.68.83:8776 check inter 2000 rise 2 fall 5 
EOF

systemctl stop haproxy
```

* Cấu hình firewall

```sh
firewall-cmd --add-port=9000/tcp --permanent
firewall-cmd --reload
```


* Stop các dịch vụ

```sh
systemctl stop httpd
# Stop mariadb lần lượt theo thứ tự từ controller 3 -> 1
systemctl stop mariadb 
```

* Khởi tạo các resource trên node controller1

```sh
pcs property set stonith-enabled=false --force
pcs resource create VIP ocf:heartbeat:IPaddr2 ip=192.168.68.84 cidr_netmask=32  op monitor interval=30s
pcs resource create HAproxy systemd:haproxy op monitor interval=2s
pcs constraint colocation add VIP with HAproxy INFINITY
#pcs resource move VIP controller1

pcs property set no-quorum-policy=ignore

pcs status
pcs resource
pcs resource show VIP

pcs resource restart HAproxy
curl 192.168.68.84
pcs resource
```

* Chỉnh lại cấu hình của mariadb trong file `/etc/my.cnf.d/mariadb-server.cnf`

```sh
...
bind-address=192.168.68.81

* Khởi động lại các service theo thứ tự từ controller 1 -> 3

```sh
systemctl start httpd
systemctl start mariadb

su -s /bin/bash keystone -c "keystone-manage db_sync"
keystone-manage bootstrap --bootstrap-password trang1234 \
 --bootstrap-admin-url http://controller:5000/v3/ \
 --bootstrap-internal-url http://controller:5000/v3/ \
 --bootstrap-public-url http://controller:5000/v3/ \
 --bootstrap-region-id RegionOne
```

* Tạo một project 

```sh
openstack project create --domain default --description "Service Project" service
```

### Cài đặt glance

* Tạo user, role,... cho Glance trong Keystone

```sh
openstack user create --domain default --project service --password trang1234 glance
openstack role add --project service --user glance admin
openstack service create --name glance --description "OpenStack Image service" image
openstack endpoint create --region RegionOne image public http://controller:9292
openstack endpoint create --region RegionOne image internal http://controller:9292
openstack endpoint create --region RegionOne image admin http://controller:9292
```

* Tạo database

```sh
mysql -u root -ptrang1234
create database glance;
grant all privileges on glance.* to glance@'localhost' identified by 'trang1234';
grant all privileges on glance.* to glance@'%' identified by 'trang1234';
flush privileges;
exit
```

* Cài đặt 

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-glance
cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.org

cat <<EOF > /etc/glance/glance-api.conf
# create new
[DEFAULT]
bind_host = 192.168.68.83

[glance_store]
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[database]
# MariaDB connection info
connection = mysql+pymysql://glance:trang1234@controller/glance

# keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = trang1234

[paste_deploy]
flavor = keystone
EOF

chmod 640 /etc/glance/glance-api.conf
chown root:glance /etc/glance/glance-api.conf

su -s /bin/bash glance -c "glance-manage db_sync"

systemctl start openstack-glance-api
systemctl enable openstack-glance-api
systemctl restart openstack-glance-api

firewall-cmd --add-port=9292/tcp --permanent
firewall-cmd --reload

wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
openstack image create "cirros" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public
```

### Cài đặt nova

* Trên Controller

```sh
openstack user create --domain default --project service --password trang1234 nova
openstack role add --project service --user nova admin
openstack user create --domain default --project service --password trang1234 placement
openstack role add --project service --user placement admin
openstack service create --name nova --description "OpenStack Compute service" compute
openstack service create --name placement --description "OpenStack Compute Placement service" placement

openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne placement public http://controller:8778
openstack endpoint create --region RegionOne placement internal http://controller:8778
openstack endpoint create --region RegionOne placement admin http://controller:8778

mysql -u root -ptrang1234
create database nova;
grant all privileges on nova.* to nova@'localhost' identified by 'trang1234';
grant all privileges on nova.* to nova@'%' identified by 'trang1234';
create database nova_api;
grant all privileges on nova_api.* to nova@'localhost' identified by 'trang1234';
grant all privileges on nova_api.* to nova@'%' identified by 'trang1234';
create database nova_cell0;
grant all privileges on nova_cell0.* to nova@'localhost' identified by 'trang1234';
grant all privileges on nova_cell0.* to nova@'%' identified by 'trang1234';
create database placement;
grant all privileges on placement.* to placement@'localhost' identified by 'trang1234';
grant all privileges on placement.* to placement@'%' identified by 'trang1234';
flush privileges;
exit


# Install 
yum --enablerepo=centos-openstack-train,epel -y install openstack-nova openstack-placement-api
cp /etc/nova/nova.conf /etc/nova/nova.conf.org

cat <<EOF > /etc/nova/nova.conf
# create new
[DEFAULT]
my_ip = 192.168.68.81
state_path = /var/lib/nova
enabled_apis = osapi_compute,metadata
log_dir = /var/log/nova
# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3
osapi_compute_listen=\$my_ip
metadata_listen=\$my_ip
metadata_host = \$my_ip
rpc_response_timeout = 180
novncproxy_host = 192.168.68.81


use_neutron = True
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
vif_plugging_is_fatal = True
vif_plugging_timeout = 300

[api]
auth_strategy = keystone

# Glance connection info
[glance]
api_servers = http://controller:9292

[oslo_concurrency]
lock_path = $state_path/tmp

# MariaDB connection info
[api_database]
connection = mysql+pymysql://nova:trang1234@controller/nova_api

[database]
connection = mysql+pymysql://nova:trang1234@controller/nova

# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[placement]
auth_url = http://controller:5000
os_region_name = RegionOne
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[wsgi]
api_paste_config = /etc/nova/api-paste.ini

[neutron]
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234
service_metadata_proxy = True
metadata_proxy_shared_secret = trang1234

[vnc]
enabled = True
server_listen = 192.168.68.81

novncproxy_base_url = http://192.168.68.84:6080/vnc_auto.html 
EOF

chmod 640 /etc/nova/nova.conf
chgrp nova /etc/nova/nova.conf

cp /etc/placement/placement.conf /etc/placement/placement.conf.org 
cat <<EOF > /etc/placement/placement.conf
[DEFAULT]
debug = false

[api]
auth_strategy = keystone

[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[placement_database]
connection = mysql+pymysql://placement:trang1234@controller/placement
EOF

chmod 640 /etc/placement/placement.conf
chgrp placement /etc/placement/placement.conf

cp /etc/httpd/conf.d/00-placement-api.conf /etc/httpd/conf.d/00-placement-api.conf.org
cat <<EOF > /etc/httpd/conf.d/00-placement-api.conf
Listen 192.168.68.81:8778

<VirtualHost 192.168.68.81:8778>
  WSGIProcessGroup placement-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIPassAuthorization On
  WSGIDaemonProcess placement-api processes=3 threads=1 user=placement group=placement
  WSGIScriptAlias / /usr/bin/placement-api
  <IfVersion >= 2.4>
    ErrorLogFormat "%M"
  </IfVersion>
  ErrorLog /var/log/placement/placement-api.log
  #SSLEngine On
  #SSLCertificateFile ...
  #SSLCertificateKeyFile ...
  <Directory /usr/bin>
    Require all granted
  </Directory>
</VirtualHost>

Alias /placement-api /usr/bin/placement-api
<Location /placement-api>
  SetHandler wsgi-script
  Options +ExecCGI
  WSGIProcessGroup placement-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIPassAuthorization On
</Location>
EOF

# Thực hiện đồng bộ lần lượt
su -s /bin/sh -c "nova-manage api_db sync" nova
su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
su -s /bin/sh -c "nova-manage db sync" nova

# Thực hiện 1 lần
su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova 

su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova

systemctl restart httpd
chown placement. /var/log/placement/placement-api.log
for service in api console conductor scheduler novncproxy; do
systemctl start openstack-nova-$service
systemctl enable openstack-nova-$service
done

for service in api console conductor scheduler novncproxy; do
systemctl restart openstack-nova-$service
systemctl status openstack-nova-$service
done

openstack compute service list
```

### Trên node compute 

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-nova-compute

cp /etc/nova/nova.conf /etc/nova/nova.conf.org

cat <<EOF > /etc/nova/nova.conf
[DEFAULT]
# define own IP
my_ip = 192.168.68.85
state_path = /var/lib/nova
enabled_apis = osapi_compute,metadata
log_dir = /var/log/nova
# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3

use_neutron = True
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
vif_plugging_is_fatal = True
vif_plugging_timeout = 300

instance_usage_audit = True
instance_usage_audit_period = hour
notify_on_state_change = vm_and_task_state
rpc_response_timeout = 180

[api]
auth_strategy = keystone

# Glance connection info
[glance]
api_servers = http://controller:9292

[oslo_concurrency]
lock_path = $state_path/tmp

# MariaDB connection info
[api_database]
connection = mysql+pymysql://nova:trang1234@controller/nova_api

[database]
connection = mysql+pymysql://nova:trang1234@controller/nova

# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[placement]
auth_url = http://controller:5000
os_region_name = RegionOne
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[wsgi]
api_paste_config = /etc/nova/api-paste.ini

[vnc]
enabled = True
server_listen = 0.0.0.0
server_proxyclient_address = 192.168.68.85
novncproxy_base_url = http://192.168.68.84:6080/vnc_auto.html 

[neutron]
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234
service_metadata_proxy = True
metadata_proxy_shared_secret = trang1234
EOF

firewall-cmd --add-port=5900-5999/tcp --permanent
firewall-cmd --reload

systemctl start openstack-nova-compute libvirtd
systemctl enable openstack-nova-compute libvirtd
systemctl restart openstack-nova-compute libvirtd

```

* Thực hiện discovery node compute trên controller

```sh
su -s /bin/bash nova -c "nova-manage cell_v2 discover_hosts"
openstack compute service list
```

* Kết quả được như sau

```sh
[root@controller1 ~(openstack)]$ openstack compute service list
+----+----------------+-------------+----------+---------+-------+----------------------------+
| ID | Binary         | Host        | Zone     | Status  | State | Updated At                 |
+----+----------------+-------------+----------+---------+-------+----------------------------+
| 13 | nova-console   | controller1 | internal | enabled | up    | 2020-02-21T01:52:24.000000 |
| 34 | nova-console   | controller2 | internal | enabled | up    | 2020-02-21T01:52:32.000000 |
| 37 | nova-console   | controller3 | internal | enabled | up    | 2020-02-21T01:52:25.000000 |
| 40 | nova-conductor | controller2 | internal | enabled | up    | 2020-02-21T01:52:27.000000 |
| 49 | nova-conductor | controller1 | internal | enabled | up    | 2020-02-21T01:52:26.000000 |
| 52 | nova-conductor | controller3 | internal | enabled | up    | 2020-02-21T01:52:29.000000 |
| 58 | nova-scheduler | controller2 | internal | enabled | up    | 2020-02-21T01:52:32.000000 |
| 61 | nova-scheduler | controller1 | internal | enabled | up    | 2020-02-21T01:52:24.000000 |
| 64 | nova-scheduler | controller3 | internal | enabled | up    | 2020-02-21T01:52:24.000000 |
| 73 | nova-compute   | compute1    | nova     | enabled | up    | 2020-02-21T01:52:25.000000 |
| 76 | nova-compute   | compute2    | nova     | enabled | up    | 2020-02-21T01:52:31.000000 |
+----+----------------+-------------+----------+---------+-------+----------------------------+
```


## Cấu hình Neutron

### Chuẩn bị cấu hình lại các card mạng trên hai node compute (network node)

```sh
cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
NAME=eth1
DEVICETYPE=ovs
TYPE=OVSPort
OVS_BRIDGE=br-eth1
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
EOF

cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-br-eth1
ONBOOT=yes
IPADDR=192.168.70.85
NETMASK=255.255.255.0
DEVICE=br-eth1
NAME=br-eth1
DEVICETYPE=ovs
OVSBOOTPROTO=none
TYPE=OVSBridge
DEFROUTE=no
EOF
```

### Trên node Controller

```sh
openstack user create --domain default --project service --password trang1234 neutron
openstack role add --project service --user neutron admin
openstack service create --name neutron --description "OpenStack Networking service" network
openstack endpoint create --region RegionOne network public http://controller:9696
openstack endpoint create --region RegionOne network internal http://controller:9696
openstack endpoint create --region RegionOne network admin http://controller:9696

mysql -u root -ptrang1234
create database neutron_ml2;
grant all privileges on neutron_ml2.* to neutron@'localhost' identified by 'trang1234';
grant all privileges on neutron_ml2.* to neutron@'%' identified by 'trang1234';
flush privileges;
exit

yum --enablerepo=centos-openstack-train,epel -y install openstack-neutron openstack-neutron-ml2

cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.org
cat <<EOF > /etc/neutron/neutron.conf
[DEFAULT]
bind_host = 192.168.68.81
core_plugin = ml2
service_plugins = router
auth_strategy = keystone
state_path = /var/lib/neutron
dhcp_agent_notification = True
allow_overlapping_ips = True
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True

# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3

l3_ha = true
max_l3_agents_per_router = 2
dhcp_agents_per_network = 2


# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = trang1234

# MariaDB connection info
[database]
connection = mysql+pymysql://neutron:trang1234@controller/neutron_ml2

# Nova connection info
[nova]
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = trang1234

[oslo_concurrency]
lock_path = \$state_path/tmp
EOF

cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.org
cat <<EOF > /etc/neutron/metadata_agent.ini
[DEFAULT]
nova_metadata_host = 192.168.68.85
#nova_metadata_ip = 192.168.68.84
metadata_proxy_shared_secret = trang1234
memcache_servers = controller1:11211, controller2:11211, controller3:11211
EOF

cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan,gre,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch
extension_drivers = port_security

[ml2_type_flat]
flat_networks = provider

[ml2_type_vxlan]
vni_ranges = 1:1000
EOF


firewall-cmd --add-port=9696/tcp --permanent
firewall-cmd --reload



ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini

su -s /bin/bash neutron -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head"

# for service in server dhcp-agent l3-agent metadata-agent openvswitch-agent; do
# systemctl start neutron-$service
# systemctl enable neutron-$service
# done

# ovs-vsctl add-br br-eth1
# ovs-vsctl add-port br-eth1 eth1

systemctl start neutron-server neutron-metadata-agent
systemctl enable neutron-server neutron-metadata-agent

systemctl restart neutron-server neutron-metadata-agent
systemctl restart openstack-nova-api
openstack network agent list
```

### Cấu hình trên compute node (các agent như l3-agent và dhcp-agent sẽ được đặt trên các compute)

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs

cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.org
cat <<EOF > /etc/neutron/neutron.conf
[DEFAULT]
core_plugin = ml2
service_plugins = router
auth_strategy = keystone
state_path = /var/lib/neutron
dhcp_agent_notification = True
allow_overlapping_ips = True

# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3

# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = trang1234

# MariaDB connection info
[database]
connection = mysql+pymysql://neutron:trang1234@controller/neutron_ml2

[nova]
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = trang1234


[oslo_concurrency]
lock_path = \$state_path/tmp
EOF

cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.org
cat <<EOF > /etc/neutron/metadata_agent.ini
[DEFAULT]
nova_metadata_host = 192.168.68.84
metadata_proxy_shared_secret = trang1234
memcache_servers = controller1:11211, controller2:11211, controller3:11211
EOF

cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan,gre,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
extension_drivers = port_security

[ml2_type_flat]
flat_networks = provider

[ml2_type_vlan]
network_vlan_ranges = provider

[ml2_type_vxlan]
vni_ranges = 1:1000
EOF

cp /etc/neutron/l3_agent.ini /etc/neutron/l3_agent.ini.org
cat <<EOF > /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = openvswitch
EOF

cp /etc/neutron/dhcp_agent.ini /etc/neutron/dhcp_agent.ini.org
cat <<EOF > /etc/neutron/dhcp_agent.ini
[DEFAULT]
interface_driver = openvswitch
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true
force_metadata = True
EOF

cp /etc/neutron/plugins/ml2/openvswitch_agent.ini /etc/neutron/plugins/ml2/openvswitch_agent.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/openvswitch_agent.ini
[securitygroup]
firewall_driver = openvswitch
enable_security_group = true
enable_ipset = true

[ovs]
local_ip = 192.168.50.85
bridge_mappings = provider:br-eth1

[agent]
tunnel_types = vxlan
prevent_arp_spoofing = True
EOF

ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
systemctl start openvswitch
systemctl enable openvswitch
ovs-vsctl add-br br-int
ovs-vsctl add-port br-eth1 eth1

for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do
systemctl start neutron-$service
systemctl enable neutron-$service
done

for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do
systemctl restart neutron-$service
done

systemctl restart network

systemctl restart openvswitch neutron-{dhcp,l3,metadata,openvswitch}-agent
```

* Kết quả:

```sh
[root@controller1 ~(openstack)]$ openstack network agent list
+----------+--------------------+-------------+-------------------+-------+-------+---------------------------+
| ID       | Agent Type         | Host        | Availability Zone | Alive | State | Binary                    |
+----------+--------------------+-------------+-------------------+-------+-------+---------------------------+
| 1f4a0c6b | L3 agent           | compute1    | nova              | :-)   | UP    | neutron-l3-agent          |
| 2764244c | Open vSwitch agent | compute1    | None              | :-)   | UP    | neutron-openvswitch-agent |
| 39c94774 | Metadata agent     | controller2 | None              | :-)   | UP    | neutron-metadata-agent    |
| 69ee8fae | Open vSwitch agent | compute2    | None              | :-)   | UP    | neutron-openvswitch-agent |
| 75c4c6c7 | DHCP agent         | compute1    | nova              | :-)   | UP    | neutron-dhcp-agent        |
| 8bb22e52 | DHCP agent         | compute2    | nova              | :-)   | UP    | neutron-dhcp-agent        |
| 945cb26c | Metadata agent     | controller1 | None              | :-)   | UP    | neutron-metadata-agent    |
| 9548fe3d | L3 agent           | compute2    | nova              | :-)   | UP    | neutron-l3-agent          |
| b382b31a | Metadata agent     | compute1    | None              | :-)   | UP    | neutron-metadata-agent    |
| d7025a3f | Metadata agent     | controller3 | None              | :-)   | UP    | neutron-metadata-agent    |
| d9e2014d | Metadata agent     | compute2    | None              | :-)   | UP    | neutron-metadata-agent    |
+----------+--------------------+-------------+-------------------+-------+-------+---------------------------+
```

### Horizon

* Cài đặt 

```sh
yum install openstack-dashboard -y
cp /etc/openstack-dashboard/local_settings /etc/openstack-dashboard/local_settings.org
```

* Chỉnh một số phần sau trong file `/etc/openstack-dashboard/local_settings`

```sh
OPENSTACK_HOST = "192.168.68.81"
ALLOWED_HOSTS = ['*']
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': ['controller1:11211','controller2:11211','controller3:11211', ]
    }
}
OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 3,
}
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"
OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
OPENSTACK_NEUTRON_NETWORK = {
    'enable_auto_allocated_network': False,
    'enable_distributed_router': False,
    'enable_fip_topology_check': True,
    'enable_ha_router': True,
    'enable_ipv6': False,
    # TODO(amotoki): Drop OPENSTACK_NEUTRON_NETWORK completely from here.
    # enable_quotas has the different default value here.
    'enable_quotas': True,
    'enable_rbac_policy': True,
    'enable_router': True,

    'default_dns_nameservers': [],
    'supported_provider_types': ['*'],
    'segmentation_id_range': {},
    'extra_provider_types': {},
    'supported_vnic_types': ['*'],
    'physical_networks': [],

}
TIME_ZONE = "Asia/Ho_Chi_Minh"
```

* Copy sang các server còn lại 

```sh
scp /etc/openstack-dashboard/local_settings  root@controller2:/etc/openstack-dashboard/
scp /etc/openstack-dashboard/local_settings  root@controller3:/etc/openstack-dashboard/
```

* Thêm dòng vào file `/etc/httpd/conf.d/openstack-dashboard.conf` 

```sh
cp /etc/httpd/conf.d/openstack-dashboard.conf /etc/httpd/conf.d/openstack-dashboard.conf.org
cat <<EOF > /etc/httpd/conf.d/openstack-dashboard.conf
WSGIDaemonProcess dashboard
WSGIProcessGroup dashboard
WSGISocketPrefix run/wsgi
WSGIApplicationGroup %{GLOBAL}

WSGIScriptAlias /dashboard /usr/share/openstack-dashboard/openstack_dashboard/wsgi/django.wsgi
Alias /dashboard/static /usr/share/openstack-dashboard/static

<Directory /usr/share/openstack-dashboard/openstack_dashboard/wsgi>
  Options All
  AllowOverride All
  Require all granted
</Directory>

<Directory /usr/share/openstack-dashboard/static>
  Options All
  AllowOverride All
  Require all granted
</Directory>
EOF
```

* Khởi động service 

```sh
systemctl restart httpd.service memcached.service
```

* Truy cập vào web browser tại http://controller/dashboard. (có thể sử dụng user của bạn hoặc sử dụng user demo mặc định với username/password là **admin**/**trang1234** domain là **default**)

* Nếu vào trình duyệt mà gặp lỗi `The requested URL /auth/login/ was not found on this server.` thì có thể thêm dòng sau vào file `/etc/openstack-dashboard/local_settings`

```sh
WEBROOT = '/dashboard/'
```


* Tạo network trên controller

```sh
#
# Create provider network
#
openstack network create  --share --external --provider-physical-network provider --provider-network-type flat provider
openstack subnet create --network provider \
  --allocation-pool start=192.168.70.91,end=192.168.70.100 \
  --dns-nameserver 8.8.8.8 --gateway 192.168.70.1 \
  --subnet-range 192.168.70.0/24 provider

#
# Create selfserivce network
#
openstack network create selfservice
openstack subnet create --network selfservice --dns-nameserver 8.8.8.8 --gateway 10.10.10.1 --subnet-range 10.10.10.0/24 selfservice


#
# Create router
#
openstack router create router
neutron router-interface-add router selfservice
neutron router-gateway-set router provider
```