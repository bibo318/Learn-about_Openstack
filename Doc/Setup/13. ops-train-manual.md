# Cài đặt OpenStack TRAIN 


## Chuẩn bị 

* Cài đặt 3 máy với các thông tin cấu hình như sau:

	* Controller 1:  

		* Manager + API: eth0 - 192.168.68.81
		* Provider: eth1 - 192.168.70.81
		* DataVM: eth2 - 192.168.50.81

	* Compute 1:
		* Manager + API: eth0 - 192.168.68.85
		* Provider: eth1 - 192.168.70.85
		* DataVM: eth2 - 192.168.50.85
	
	* Compute 2:
		* Manager + API: eth0 - 192.168.68.86
		* Provider: eth1 - 192.168.70.86
		* DataVM: eth2 - 192.168.50.86

	* Cinder
		* Manager + API: eth0 - 192.168.68.87


* Set hostname và timezone

```sh
hostnamectl set-hostname controller1
HOSTNAME=controller1
timedatectl set-timezone Asia/Ho_Chi_Minh
```

* Cấu hình file host như sau:

```sh
cat <<EOF >> /etc/hosts

192.168.68.81 controller1
192.168.68.82 controller2
192.168.68.83 controller3
192.168.68.84 controller # VIP

192.168.68.85 compute1
192.168.68.86 compute2
192.168.68.87 cinder1
EOF
```

* Cấu hình selinux

```sh
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config
setenforce 0
```

* Cấu hình IP trên các node tương tự trên controller 1 như sau:

```sh
cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-eth0
# Created by cloud-init on instance boot automatically, do not edit.
#
BOOTPROTO=none
DEVICE=eth0
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
IPADDR=192.168.68.81
PREFIX=24
GATEWAY=192.168.68.1
DNS1=8.8.8.8
EOF

cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-eth1
BOOTPROTO=none
DEVICE=eth1
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
IPADDR=192.168.70.81
PREFIX=24
EOF

cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-eth2
BOOTPROTO=none
DEVICE=eth2
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
IPADDR=192.168.50.81
PREFIX=24
EOF
```

* Khởi động lại network 

```sh
systemctl restart network
```

* Cấu hình chrony trên các node controller

```sh
yum install chrony -y
sed -i "s/server 0.centos.pool.ntp.org iburst/#server 0.centos.pool.ntp.org iburst/g" /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed  -i '1i server vn.pool.ntp.org iburst' /etc/chrony.conf
sed  -i '2i allow 192.168.68.0\/24' /etc/chrony.conf

systemctl enable chronyd.service
systemctl start chronyd.service
chronyc sources
```

* Các node khác:

```sh
yum install chrony -y
sed -i "s/server 0.centos.pool.ntp.org iburst/#server 0.centos.pool.ntp.org iburst/g" /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed  -i '1i server controller iburst' /etc/chrony.conf

systemctl enable chronyd.service
systemctl start chronyd.service
```

* Kiểm tra xem đã đồng bộ được thời gian giữa các node chưa

```sh
firewall-cmd --add-service=ntp --permanent
firewall-cmd --reload
chronyc sources
```

### Cài đặt ban đầu

* Cài đặt Repository của Openstack Train

```sh
yum -y install centos-release-openstack-train
```

### Cài đặt mariadb giữa các node

* Trên node controller (chú ý một vài chỗ cần đổi ip và tên node trên từng node)

```sh
yum --enablerepo=centos-openstack-train -y install mariadb-server

cat <<EOF > /etc/my.cnf.d/openstack.cnf
[mysqld]

# ip controller
bind-address = 192.168.68.81
default-storage-engine = innodb
innodb_file_per_table
max_connections = 1024
collation-server = utf8_general_ci
character-set-server = utf8
EOF

mysql_secure_installation
```

* Chỉnh sửa file `/etc/my.cnf.d/mariadb-server.cnf`

```sh
# add into [mysqld] section
[mysqld]
.....
.....
# default value 151 is not enough on Openstack Env
max_connections=500
# because sometimes it happens errors with utf8mb4 on Openstack DB
character-set-server=utf8 
```

* Khởi động lại service 

```sh
systemctl enable mariadb.service
systemctl start mariadb.service
```


### Cài đặt rabbitmq và memcache

```sh
yum --enablerepo=centos-openstack-train -y install rabbitmq-server memcached
systemctl start rabbitmq-server memcached 
systemctl enable rabbitmq-server memcached

rabbitmqctl add_user openstack trang1234
rabbitmqctl set_permissions openstack ".*" ".*" ".*"

cp /etc/sysconfig/memcached /etc/sysconfig/memcached.origin
sed -i 's/OPTIONS=\"-l 127.0.0.1,::1\"/OPTIONS=\"-l 192.168.68.81\"/g' /etc/sysconfig/memcached
systemctl restart rabbitmq-server memcached
```

* Allow ports for service firewall

```sh
firewall-cmd --add-service=mysql --permanent
firewall-cmd --add-port={11211/tcp,5672/tcp} --permanent
firewall-cmd --reload
```

## 1. Cài đặt Keystone

* Tạo database cho keystone

```sh
mysql -u root -ptrang1234
create database keystone;
grant all privileges on keystone.* to keystone@'localhost' identified by 'trang1234';
grant all privileges on keystone.* to keystone@'%' identified by 'trang1234';
flush privileges;
exit
```

* Cài đặt keystone và cấu hình

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi
cp /etc/keystone/keystone.conf /etc/keystone/keystone.conf.orig

cat <<EOF > /etc/keystone/keystone.conf
[DEFAULT]
strict_password_check = true
max_logfile_count = 60
max_logfile_size_mb = 300
log_rotation_type = size
admin_token = 18a5142504aae35b8de4
# log_config_append = /etc/keystone/logging.conf
[cache]
memcache_servers = controller1:11211
memcache_dead_retry = 10
memcache_socket_timeout = 1.0
memcache_pool_maxsize = 1024
memcache_pool_unused_timeout = 10
[database]
connection = mysql+pymysql://keystone:trang1234@controller1/keystone
use_db_reconnect  = true
connection_recycle_time = 3600
max_overflow = 50
max_pool_size = 20
db_retry_interval = 2
db_max_retries = -1

[token]
provider = fernet
expiration = 43200

[fernet_tokens]
key_repository = /etc/keystone/fernet-keys/
max_active_keys = 8

#[security_compliance]
#lockout_duration = 1800
#lockout_failure_attempts = 5
#password_regex = ^(?=.*\d)(?=.*[a-z])(?=.*[A-Z])(?=.*[!@#$%^&*()<>{}+=_\\\[\]\-?|~`,.;:]).{7,}$
#password_regex_description = Password must have a minimum length of 7 characters, and must contain at least 1 upper case, 1 lower case, 1 digit, and 1 special character
#unique_last_password_count = 2
EOF

su -s /bin/sh -c "keystone-manage db_sync" keystone
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
keystone-manage bootstrap --bootstrap-password trang1234 \
--bootstrap-admin-url http://controller1:5000/v3/ \
--bootstrap-internal-url http://controller1:5000/v3/ \
--bootstrap-public-url http://controller1:5000/v3/ \
--bootstrap-region-id RegionOne

cp -np /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.origin
echo "ServerName `hostname`" >> /etc/httpd/conf/httpd.conf
sed -i -e "s/^Listen.*/Listen `hostname -i`:80/g" /etc/httpd/conf/httpd.conf
systemctl enable httpd.service
systemctl start httpd.service

ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 
systemctl restart httpd 

firewall-cmd --add-port=5000/tcp --permanent
firewall-cmd --reload

cat <<EOF > ~/keystonerc
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=trang1234
export OS_AUTH_URL=http://controller1:5000/v3
export OS_IDENTITY_API_VERSION=3
export PS1='[\u@\h \W(openstack)]\$ '
export OS_AUTH_TYPE=password
EOF

chmod 600 ~/keystonerc 
source ~/keystonerc 
echo "source ~/keystonerc " >> ~/.bash_profile
```

* Tạo một project

```sh
openstack project create --domain default --description "Service Project" service
openstack project list
```

## 2. Cài đặt Glance

* Tạo user, role,... cho Glance trong Keystone

```sh
openstack user create --domain default --project service --password trang1234 glance
openstack role add --project service --user glance admin
openstack service create --name glance --description "OpenStack Image service" image
openstack endpoint create --region RegionOne image public http://controller1:9292
openstack endpoint create --region RegionOne image internal http://controller1:9292
openstack endpoint create --region RegionOne image admin http://controller1:9292
```

* Tạo database

```sh
mysql -u root -ptrang1234
create database glance;
grant all privileges on glance.* to glance@'localhost' identified by 'trang1234';
grant all privileges on glance.* to glance@'%' identified by 'trang1234';
flush privileges;
exit
```

* Cài đặt 

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-glance
cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.org

cat <<EOF > /etc/glance/glance-api.conf
# create new
[DEFAULT]
bind_host = 192.168.68.81

[glance_store]
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[database]
# MariaDB connection info
connection = mysql+pymysql://glance:trang1234@controller1/glance

# keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = trang1234

[paste_deploy]
flavor = keystone
EOF

chmod 640 /etc/glance/glance-api.conf
chown root:glance /etc/glance/glance-api.conf
su -s /bin/bash glance -c "glance-manage db_sync"
systemctl start openstack-glance-api
systemctl enable openstack-glance-api

firewall-cmd --add-port=9292/tcp --permanent
firewall-cmd --reload

wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
openstack image create "cirros" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public
```

## 3. Cài đặt Nova

### Controller

```sh
openstack user create --domain default --project service --password trang1234 nova
openstack role add --project service --user nova admin
openstack user create --domain default --project service --password trang1234 placement
openstack role add --project service --user placement admin
openstack service create --name nova --description "OpenStack Compute service" compute
openstack service create --name placement --description "OpenStack Compute Placement service" placement

openstack endpoint create --region RegionOne compute public http://controller1:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne compute internal http://controller1:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne compute admin http://controller1:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne placement public http://controller1:8778
openstack endpoint create --region RegionOne placement internal http://controller1:8778
openstack endpoint create --region RegionOne placement admin http://controller1:8778

mysql -u root -ptrang1234
create database nova;
grant all privileges on nova.* to nova@'localhost' identified by 'trang1234';
grant all privileges on nova.* to nova@'%' identified by 'trang1234';
create database nova_api;
grant all privileges on nova_api.* to nova@'localhost' identified by 'trang1234';
grant all privileges on nova_api.* to nova@'%' identified by 'trang1234';
create database nova_cell0;
grant all privileges on nova_cell0.* to nova@'localhost' identified by 'trang1234';
grant all privileges on nova_cell0.* to nova@'%' identified by 'trang1234';
create database placement;
grant all privileges on placement.* to placement@'localhost' identified by 'trang1234';
grant all privileges on placement.* to placement@'%' identified by 'trang1234';
flush privileges;
exit

# Install 
yum --enablerepo=centos-openstack-train,epel -y install openstack-nova openstack-placement-api
cp /etc/nova/nova.conf /etc/nova/nova.conf.org

cat <<EOF > /etc/nova/nova.conf
# create new
[DEFAULT]
# define own IP
my_ip = 192.168.68.81
state_path = /var/lib/nova
enabled_apis = osapi_compute,metadata
log_dir = /var/log/nova
# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1

use_neutron = True
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
vif_plugging_is_fatal = True
vif_plugging_timeout = 300

[api]
auth_strategy = keystone

# Glance connection info
[glance]
api_servers = http://controller1:9292

[oslo_concurrency]
lock_path = $state_path/tmp

# MariaDB connection info
[api_database]
connection = mysql+pymysql://nova:trang1234@controller1/nova_api

[database]
connection = mysql+pymysql://nova:trang1234@controller1/nova

# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[placement]
auth_url = http://controller1:5000
os_region_name = RegionOne
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[wsgi]
api_paste_config = /etc/nova/api-paste.ini

[neutron]
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234
service_metadata_proxy = True
metadata_proxy_shared_secret = trang1234
EOF

chmod 640 /etc/nova/nova.conf
chgrp nova /etc/nova/nova.conf

cp /etc/placement/placement.conf /etc/placement/placement.conf.org 
cat <<EOF > /etc/placement/placement.conf
[DEFAULT]
debug = false

[api]
auth_strategy = keystone

[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[placement_database]
connection = mysql+pymysql://placement:trang1234@controller1/placement
EOF

chmod 640 /etc/placement/placement.conf
chgrp placement /etc/placement/placement.conf
```

* Thêm 1 số dòng sau vào file `/etc/httpd/conf.d/00-placement-api.conf`

```sh
#dòng 15
  <Directory /usr/bin>
    Require all granted
  </Directory>
</VirtualHost>
```

* Nếu firewall đang hoạt động

```sh
firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent
firewall-cmd --reload
```

* Add Data vào database và start Nova service 

```sh
su -s /bin/bash placement -c "placement-manage db sync"
su -s /bin/bash nova -c "nova-manage api_db sync"

su -s /bin/bash nova -c "nova-manage cell_v2 map_cell0"
su -s /bin/bash nova -c "nova-manage db sync"
su -s /bin/bash nova -c "nova-manage cell_v2 create_cell --name cell1"

systemctl restart httpd
chown placement. /var/log/placement/placement-api.log
for service in api console conductor scheduler novncproxy; do
systemctl start openstack-nova-$service
systemctl enable openstack-nova-$service
done

for service in api console conductor scheduler novncproxy; do
systemctl restart openstack-nova-$service
systemctl status openstack-nova-$service
done

openstack compute service list
```

### Trên node compute 

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-nova-compute

cp /etc/nova/nova.conf /etc/nova/nova.conf.org

cat <<EOF > /etc/nova/nova.conf
[DEFAULT]
# define own IP
my_ip = 192.168.68.85
state_path = /var/lib/nova
enabled_apis = osapi_compute,metadata
log_dir = /var/log/nova
# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1

use_neutron = True
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
vif_plugging_is_fatal = True
vif_plugging_timeout = 300

[api]
auth_strategy = keystone

# Glance connection info
[glance]
api_servers = http://controller1:9292

[oslo_concurrency]
lock_path = $state_path/tmp

# MariaDB connection info
[api_database]
connection = mysql+pymysql://nova:trang1234@controller1/nova_api

[database]
connection = mysql+pymysql://nova:trang1234@controller1/nova

# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[placement]
auth_url = http://controller1:5000
os_region_name = RegionOne
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[wsgi]
api_paste_config = /etc/nova/api-paste.ini

[vnc]
enabled = True
server_listen = 0.0.0.0
server_proxyclient_address = 192.168.68.85
novncproxy_base_url = http://192.168.68.81:6080/vnc_auto.html 

[neutron]
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234
service_metadata_proxy = True
metadata_proxy_shared_secret = trang1234
EOF

firewall-cmd --add-port=5900-5999/tcp --permanent
firewall-cmd --reload

modprobe br_netfilter
cat <<EOF > /etc/sysctl.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl -p

systemctl enable libvirtd.service openstack-nova-compute.service
systemctl restart libvirtd.service openstack-nova-compute.service
```

* Thực hiện discovery node compute trên controller

```sh
su -s /bin/bash nova -c "nova-manage cell_v2 discover_hosts"
openstack compute service list
openstack compute service list --service nova-compute
```

## 4. Cấu hình Neutron

### Chuẩn bị cấu hình lại các card mạng trên hai node compute (network node)

```sh
cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
NAME=eth1
DEVICETYPE=ovs
TYPE=OVSPort
OVS_BRIDGE=br-eth1
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
EOF

cat <<EOF >/etc/sysconfig/network-scripts/ifcfg-br-eth1
ONBOOT=yes
IPADDR=192.168.70.86
NETMASK=255.255.255.0
DEVICE=br-eth1
NAME=br-eth1
DEVICETYPE=ovs
OVSBOOTPROTO=none
TYPE=OVSBridge
DEFROUTE=no
EOF
```

### Trên node Controller

```sh
openstack user create --domain default --project service --password trang1234 neutron
openstack role add --project service --user neutron admin
openstack service create --name neutron --description "OpenStack Networking service" network
openstack endpoint create --region RegionOne network public http://controller1:9696
openstack endpoint create --region RegionOne network internal http://controller1:9696
openstack endpoint create --region RegionOne network admin http://controller1:9696

mysql -u root -ptrang1234
create database neutron_ml2;
grant all privileges on neutron_ml2.* to neutron@'localhost' identified by 'trang1234';
grant all privileges on neutron_ml2.* to neutron@'%' identified by 'trang1234';
flush privileges;
exit

yum --enablerepo=centos-openstack-train,epel -y install openstack-neutron openstack-neutron-ml2

cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.org
cat <<EOF > /etc/neutron/neutron.conf
[DEFAULT]
core_plugin = ml2
service_plugins = router
auth_strategy = keystone
state_path = /var/lib/neutron
dhcp_agent_notification = True
allow_overlapping_ips = True
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True

# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1

l3_ha = true
max_l3_agents_per_router = 2

dhcp_agents_per_network = 2


# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = trang1234

# MariaDB connection info
[database]
connection = mysql+pymysql://neutron:trang1234@controller1/neutron_ml2

# Nova connection info
[nova]
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = trang1234

[oslo_concurrency]
lock_path = $state_path/tmp
EOF

cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.org
cat <<EOF > /etc/neutron/metadata_agent.ini
[DEFAULT]
nova_metadata_host = controller1
metadata_proxy_shared_secret = metadata_secret
memcache_servers = controller1:11211
EOF

cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan,gre,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch
extension_drivers = port_security

[ml2_type_flat]
flat_networks = provider

[ml2_type_vxlan]
vni_ranges = 1:1000
EOF


firewall-cmd --add-port=9696/tcp --permanent
firewall-cmd --reload



ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
su -s /bin/bash neutron -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head"

# for service in server dhcp-agent l3-agent metadata-agent openvswitch-agent; do
# systemctl start neutron-$service
# systemctl enable neutron-$service
# done

# ovs-vsctl add-br br-eth1
# ovs-vsctl add-port br-eth1 eth1

systemctl restart neutron-server neutron-metadata-agent
systemctl enable neutron-server neutron-metadata-agent

systemctl restart openstack-nova-api openstack-nova-compute
openstack network agent list
```

### Cấu hình trên compute node (các agent như l3-agent và dhcp-agent sẽ được đặt trên các compute)

```sh
yum --enablerepo=centos-openstack-train,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs

cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.org
cat <<EOF > /etc/neutron/neutron.conf
[DEFAULT]
core_plugin = ml2
service_plugins = router
auth_strategy = keystone
state_path = /var/lib/neutron
dhcp_agent_notification = True
allow_overlapping_ips = True

# RabbitMQ connection info
transport_url = rabbit://openstack:trang1234@controller1

# Keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = trang1234

# MariaDB connection info
[database]
connection = mysql+pymysql://neutron:trang1234@controller1/neutron_ml2

[nova]
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = trang1234


[oslo_concurrency]
lock_path = $state_path/tmp
EOF

cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.org
cat <<EOF > /etc/neutron/metadata_agent.ini
[DEFAULT]
nova_metadata_host = controller1
metadata_proxy_shared_secret = metadata_secret
memcache_servers = controller1:11211
EOF

cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan,gre,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
extension_drivers = port_security

[ml2_type_flat]
flat_networks = provider

[ml2_type_vlan]
network_vlan_ranges = provider

[ml2_type_vxlan]
vni_ranges = 1:1000
EOF

cp /etc/neutron/l3_agent.ini /etc/neutron/l3_agent.ini.org
cat <<EOF > /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = openvswitch
EOF

cp /etc/neutron/dhcp_agent.ini /etc/neutron/dhcp_agent.ini.org
cat <<EOF > /etc/neutron/dhcp_agent.ini
[DEFAULT]
interface_driver = openvswitch
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true
force_metadata = True
EOF

cp /etc/neutron/plugins/ml2/openvswitch_agent.ini /etc/neutron/plugins/ml2/openvswitch_agent.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/openvswitch_agent.ini
[securitygroup]
firewall_driver = openvswitch
enable_security_group = true
enable_ipset = true

[ovs]
local_ip = 192.168.50.85
bridge_mappings = provider:br-eth1

[agent]
tunnel_types = vxlan
prevent_arp_spoofing = True
EOF

ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
systemctl start openvswitch
systemctl enable openvswitch
ovs-vsctl add-br br-int
ovs-vsctl add-port br-eth1 eth1

for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do
systemctl start neutron-$service
systemctl enable neutron-$service
done

for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do
systemctl restart neutron-$service
done

systemctl restart network

systemctl restart openvswitch neutron-{dhcp,l3,metadata,openvswitch}-agent
```


* Nếu cài đặt thành công, trên controller node sẽ thấy được các agent hiện có như sau:

```sh
[root@controller1 ~(openstack)]$ openstack network agent list
+-----------------+----------------+-------------+-------------------+-------+-------+------------------------+
| ID              | Agent Type     | Host        | Availability Zone | Alive | State | Binary                 |
+-----------------+----------------+-------------+-------------------+-------+-------+------------------------+
| ...ebb8b4701642 | Metadata agent | compute1    | None              | :-)   | UP    | neutron-metadata-agent |
| ...b35aacbaea55 | L3 agent       | compute1    | nova              | :-)   | UP    | neutron-l3-agent       |
| ...fdd32d6f4bf0 | DHCP agent     | compute2    | nova              | :-)   | UP    | neutron-dhcp-agent     |
| ...7f20e6537a40 | DHCP agent     | compute1    | nova              | :-)   | UP    | neutron-dhcp-agent     |
| ...dbed6d099afb | Metadata agent | controller1 | None              | :-)   | UP    | neutron-metadata-agent |
| ...623aaa64fb13 | L3 agent       | compute2    | nova              | :-)   | UP    | neutron-l3-agent       |
| ...8729b3bc99d2 | Metadata agent | compute2    | None              | :-)   | UP    | neutron-metadata-agent |
+-----------------+----------------+-------------+-------------------+-------+-------+------------------------+
```



### Trên controller node

```sh
#
# Create provider network
#
openstack network create  --share --external --provider-physical-network provider --provider-network-type flat provider
openstack subnet create --network provider \
  --allocation-pool start=192.168.70.91,end=192.168.70.100 \
  --dns-nameserver 8.8.8.8 --gateway 192.168.70.1 \
  --subnet-range 192.168.70.0/24 provider

#
# Create selfserivce network
#
openstack network create selfservice
openstack subnet create --network selfservice --dns-nameserver 8.8.8.8 --gateway 10.10.10.1 --subnet-range 10.10.10.0/24 selfservice


#
# Create router
#
openstack router create router
neutron router-interface-add router selfservice
neutron router-gateway-set router provider
```

* Kiểm tra lại xem các cấu hình HA cho l3-agent và dhcp-agent đã được chưa

```sh
[root@controller1 ~(openstack)]$ openstack network list
+------------------+----------------------------------------------------+-----------------------------------+
| ID               | Name                                               | Subnets                           |
+------------------+----------------------------------------------------+-----------------------------------+
| 3fd-b225f5030f9c | selfservice                                        | b94f0e17-ff61-4-ac64-eef0d5fdf84e |
| b2b-9b55f4c2e257 | provider                                           | ce8faa14-52bb-4-9fee-bd8deead4433 |
| d54-7c1f7cf6e2ff | HA network tenant 9b0c528bd7274a4aab9ff68b9049f81c | aa9997ff-10a2-4-b9a7-8dc2e2c803d0 |
+------------------+----------------------------------------------------+-----------------------------------+
[root@controller1 ~(openstack)]$ openstack router list
+------------------+--------+--------+-------+----------------------------------+-------------+------+
| ID               | Name   | Status | State | Project                          | Distributed | HA   |
+------------------+--------+--------+-------+----------------------------------+-------------+------+
| 444-5b8deeeb9444 | router | ACTIVE | UP    | 9b0c528bd7274a4aab9ff68b9049f81c | False       | True |
+------------------+--------+--------+-------+----------------------------------+-------------+------+

[root@controller1 ~(openstack)]$ ip netns
[root@controller1 ~(openstack)]$ openstack subnet list
+-------------+---------------------------------------------------+-------------------------+------------------+
| ID          | Name                                              | Network                 | Subnet           |
+-------------+---------------------------------------------------+-------------------------+------------------+
| aa..2c803d0 | HA subnet tenant 9b0c528bd7274a4aab9ff68b9049f81c | d54f0e6..f-7c1f7cf6e2ff | 169.254.192.0/18 |
| b9..5fdf84e | selfservice                                       | 3fda5de..8-b225f5030f9c | 10.10.10.0/24    |
| ce..ead4433 | provider                                          | b2bb602..c-9b55f4c2e257 | 192.168.70.0/24  |
+-------------+---------------------------------------------------+-------------------------+------------------+


[root@compute1 network-scripts]# ip netns
qdhcp-3fda5de5-8534-4b1c-a919-b225f5030f9c (id: 2)
qdhcp-b2bb602b-cb1a-42a6-8c36-9b55f4c2e257 (id: 1)
qrouter-44440548-4e52-4be0-8cf8-5b8deeeb9444 (id: 0)
[root@compute1 network-scripts]# ip netns exec qrouter-44440548-4e52-4be0-8cf8-5b8deeeb9444 ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
8: ha-7c036318-98: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether fa:16:3e:06:f5:37 brd ff:ff:ff:ff:ff:ff
    inet 169.254.192.111/18 brd 169.254.255.255 scope global ha-7c036318-98
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe06:f537/64 scope link
       valid_lft forever preferred_lft forever
9: qr-3b35d14f-98: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether fa:16:3e:8d:f3:79 brd ff:ff:ff:ff:ff:ff
10: qg-76c79fdc-a6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether fa:16:3e:eb:34:f1 brd ff:ff:ff:ff:ff:ff

[root@compute2 ~]# ip netns
qdhcp-3fda5de5-8534-4b1c-a919-b225f5030f9c (id: 2)
qdhcp-b2bb602b-cb1a-42a6-8c36-9b55f4c2e257 (id: 1)
qrouter-44440548-4e52-4be0-8cf8-5b8deeeb9444 (id: 0)
[root@compute2 ~]# ip netns exec qrouter-44440548-4e52-4be0-8cf8-5b8deeeb9444 ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
7: ha-6d34acaa-e7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether fa:16:3e:2f:90:3f brd ff:ff:ff:ff:ff:ff
    inet 169.254.192.55/18 brd 169.254.255.255 scope global ha-6d34acaa-e7
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe2f:903f/64 scope link
       valid_lft forever preferred_lft forever
8: qr-3b35d14f-98: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether fa:16:3e:8d:f3:79 brd ff:ff:ff:ff:ff:ff
9: qg-76c79fdc-a6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether fa:16:3e:eb:34:f1 brd ff:ff:ff:ff:ff:ff
```



### Một số các thao tác khác


* Create virtual network 

```sh
projectID=$(openstack project list | grep service | awk '{print $2}')
openstack network create --project $projectID --share --provider-network-type flat --provider-physical-network provider provider

openstack subnet create subnet-provider --network provider \
--project $projectID --subnet-range 192.168.70.0/24 \
--allocation-pool start=192.168.70.90,end=192.168.70.100 \
--gateway 192.168.70.1 --dns-nameserver 8.8.8.8

openstack network list
openstack subnet list
```

* Tạo user

```sh
openstack project create --domain default --description "MediTech Project" meditech
openstack user create --domain default --project meditech --password trang1234 trangnth
openstack role create CloudUser
openstack role add --project meditech --user trangnth CloudUser

# Tạo flavor
openstack flavor create --id 0 --vcpus 1 --ram 2048 --disk 10 m1.small
```

* Tạo một security group 

```sh
openstack security group create secgroup01
openstack security group list
```

* Tạo keypair để connect tới instance

```sh
ssh-keygen -q -N ""

openstack keypair create --public-key ~/.ssh/id_rsa.pub trang_key
openstack keypair list

# Ví dụ tạo một instance với keypair vừa tạo
netID=$(openstack network list | grep sharednet1 | awk '{ print $2 }')
openstack server create --flavor m1.small --image CentOS7 --security-group secgroup01 --nic net-id=$netID --key-name trang_key CentOS_7
openstack server stop CentOS_7
openstack server start CentOS_7
openstack console url show CentOS_7
```

* Thêm rule cho security group 

```sh
# permit ICMP
openstack security group rule create --protocol icmp --ingress secgroup01

# permit SSH
openstack security group rule create --protocol tcp --dst-port 22:22 secgroup01

openstack security group rule list
```


## 5. Horizon

* Cài đặt 

```sh
yum install openstack-dashboard -y
```

* Chỉnh một số phần sau trong file `/etc/openstack-dashboard/local_settings`

```sh
OPENSTACK_HOST = "192.168.68.81"
ALLOWED_HOSTS = ['*']
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': 'controller1:11211',
    }
}
OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 3,
}
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"
OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
OPENSTACK_NEUTRON_NETWORK = {
    'enable_auto_allocated_network': False,
    'enable_distributed_router': False,
    'enable_fip_topology_check': True,
    'enable_ha_router': True,
    'enable_ipv6': False,
    # TODO(amotoki): Drop OPENSTACK_NEUTRON_NETWORK completely from here.
    # enable_quotas has the different default value here.
    'enable_quotas': True,
    'enable_rbac_policy': True,
    'enable_router': True,

    'default_dns_nameservers': [],
    'supported_provider_types': ['*'],
    'segmentation_id_range': {},
    'extra_provider_types': {},
    'supported_vnic_types': ['*'],
    'physical_networks': [],

}
TIME_ZONE = "Asia/Ho_Chi_Minh"
```

* Thêm dòng sau vào file `/etc/httpd/conf.d/openstack-dashboard.conf` nếu trong file ko có

```sh
WSGIApplicationGroup %{GLOBAL}
```

* Khởi động service 

```sh
systemctl restart httpd.service memcached.service
```

* Truy cập vào web browser tại http://controller1/dashboard. (có thể sử dụng user của bạn hoặc sử dụng user demo mặc định với username/password là **admin**/**demo** domain là **default**)

* Nếu vào trình duyệt mà gặp lỗi `The requested URL /auth/login/ was not found on this server.` thì có thể thêm dòng sau vào file `/etc/openstack-dashboard/local_settings`

```sh
WEBROOT = '/dashboard/'
```


* Tham khảo thêm: https://docs.openstack.org/horizon/train/admin/customize-configure.html