# Ghi chép các bước cấu hình OPS HA 3 node controleler

## 1. Chuẩn bị môi trường 

Gồm 3 node controller:

* Controller 1

```sh
eth0: 192.168.40.71  - PROVIDER
eth1: 192.168.50.71  - DATAVM
eth2: 192.168.68.71  - MANAGERMENT
```

* Controller 2

```sh
eth0: 192.168.40.76
eth1: 192.168.50.76
eth2: 192.168.68.76
```

* Controller 3

```sh
eth0: 192.168.40.77
eth1: 192.168.50.77
eth2: 192.168.68.77
```

Hai node Compute:

* Compute1

```sh
eth0: 192.168.40.72  - PROVIDER
eth1: 192.168.50.72  - DATAVM
eth2: 192.168.68.72  - MANAGERMENT
```

* Compute2

```sh
eth0: 192.168.40.73  - PROVIDER
eth1: 192.168.50.73  - DATAVM
eth2: 192.168.68.73  - MANAGERMENT
```

## Thiết lập môi trường ban đầu trên tất cả các node

hostname, NTP, IP,....

```sh
hostnamectl set-hostname controller3
systemctl disable firewalld
systemctl stop firewalld
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

yum install chrony -y
sed -i "s/server 0.centos.pool.ntp.org iburst/server 192.168.40.71 iburst/g" /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/#allow 192.168.0.0\/16/allow 192.168.40.0\/24/g' /etc/chrony.conf

systemctl enable chronyd.service
systemctl start chronyd.service

chronyc sources

cat << EOF >/etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.40.71   controller1 
192.168.40.76   controller2
192.168.40.77   controller3
192.168.40.78   controller
192.168.40.72   compute1
192.168.40.73   compute2
192.168.40.74   storage
EOF

yum -y install centos-release-openstack-rocky 
sed -i -e "s/enabled=1/enabled=0/g" /etc/yum.repos.d/CentOS-OpenStack-rocky.repo 
```

## Trên cả ba node controller

* Cài đặt mariadb

```sh
yum --enablerepo=centos-openstack-rocky -y install mariadb-server

cat <<EOF > /etc/my.cnf.d/openstack.cnf
[mysqld]

bind-address = 192.168.40.77
default-storage-engine = innodb
innodb_file_per_table
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8
EOF

systemctl enable mariadb.service
systemctl start mariadb.service


yum --enablerepo=centos-openstack-rocky -y install galera rsync mariadb-server-galera.x86_64

cat <<EOF > /etc/my.cnf.d/galera.cnf
[galera]
# Mandatory settings
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so

#add your node ips here
wsrep_cluster_address="gcomm://192.168.40.71,192.168.40.76,192.168.40.77"
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
#Cluster name

wsrep_cluster_name="galera_cluster"
# Allow server to accept connections on all interfaces.

bind-address=192.168.40.77

# this server ip, change for each server
wsrep_node_address="192.168.40.77"
# this server name, change for each server
wsrep_node_name="node3"

wsrep_sst_method=rsync
EOF

# Tạo mật khẩu trên một node controller1
mysql_secure_installation
```

* Nhập mật khẩu cho user root và tiếp tục cấu hình galera như sau:

```sh
# Thực hiện trên controller 1
galera_new_cluster
cat /var/lib/mysql/grastate.dat

# Khởi động lại service trên tất cả các node controller theo thứ tự lần lượt, controller1 -> controller 2 -> controller3
systemctl restart mariadb
mysql -u root -ptrang1234 -e "SHOW STATUS LIKE 'wsrep_cluster_size'"
```

* Cấu hình cluster check trên tất cả các con controller

```sh
yum install -y xinetd wget
rm -f clustercheck ; rm -f  /usr/bin/clustercheck
wget  https://raw.githubusercontent.com/hungnt1/percona-clustercheck/master/clustercheck
mv  clustercheck /usr/bin && chmod 555 /usr/bin/clustercheck

mysql -u root -ptrang1234 -e "GRANT PROCESS ON *.* TO 'clustercheckuser'@'localhost' IDENTIFIED BY '123@123Aa'"

cat <<EOF>  /etc/xinetd.d/mysqlchk
# default: on
# description: mysqlchk
service mysqlchk
{
        disable = no
        flags = REUSE
        socket_type = stream
        port = 9200
        wait = no
        user = nobody
        server = /usr/bin/clustercheck
        log_on_failure += USERID
        only_from = 0.0.0.0/0
        per_source = UNLIMITED
}
EOF
echo 'mysqlchk 9200/tcp # MySQL check' >> /etc/services
systemctl start xinetd
systemctl enable xinetd
```


* Cài đặt rabbitmq, memcache trên cả ba con controller

```sh
# Trên cả ba node
yum --enablerepo=epel -y install rabbitmq-server memcached
systemctl start rabbitmq-server memcached 
systemctl enable rabbitmq-server memcached

rabbitmqctl add_user openstack trang1234
rabbitmqctl set_permissions openstack ".*" ".*" ".*"

cp /etc/sysconfig/memcached /etc/sysconfig/memcached.origin
sed -i 's/OPTIONS=\"\"/OPTIONS=\"-l 192.168.40.77,::1\"/g' /etc/sysconfig/memcached
systemctl restart rabbitmq-server memcached

# enable plugin và copy erlang cookie từ controller1 sang các node còn lại
# Chú ý thực hiện lệnh lần lượt
rabbitmq-plugins enable rabbitmq_management  
rabbitmq-server -detached  

scp  /var/lib/rabbitmq/.erlang.cookie root@controller2:/var/lib/rabbitmq/
scp  /var/lib/rabbitmq/.erlang.cookie root@controller3:/var/lib/rabbitmq/

# Thực hiện join cluster trên controller 2 và 3
sudo systemctl restart rabbitmq-server
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@controller1
rabbitmqctl start_app
rabbitmqctl cluster_status

# Cấu hình HA policy trên 3 npde Controller 
rabbitmqctl set_policy ha-all '^(?!amq\.).*' '{"ha-mode": "all"}' 
```

* Kiểm tra lại rabbitmq cluster  trên controller1 như sau:

```sh
[root@controller1 ~(openstack)]# rabbitmqctl cluster_status
Cluster status of node rabbit@controller ...
[{nodes,[{disc,[rabbit@controller1,rabbit@controller2,rabbit@controller3]}]},
 {running_nodes,[rabbit@controller2,rabbit@controller3,rabbit@controller1]},
 {cluster_name,<<"rabbit@controller1">>},
 {partitions,[]}]
...done.
```

* Cài đặt keystone

```sh
# Thực hiện trên controller1
mysql -u root -ptrang1234
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'trang1234';
flush privileges; 
exit


# Cài đặt trân cả ba node
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi
cp /etc/keystone/keystone.conf /etc/keystone/keystone.conf.org

cat <<EOF > /etc/keystone/keystone.conf
[cache]
memcache_servers = controller1:11211,controller2:11211,controller3:11211

[cors]
allowed_origin = http://192.168.40.129:3000
allow_methods = GET,PUT,POST,DELETE,PATCH
allow_headers = X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-Domain-Id,X-Domain-Name

[database]
connection = mysql+pymysql://keystone:trang1234@controller/keystone

[token]
provider = fernet
EOF

su -s /bin/bash keystone -c "keystone-manage db_sync"

cat<<EOF > /etc/httpd/conf.d/wsgi-keystone.conf
Listen 192.168.40.76:5000

<VirtualHost 192.168.40.76:5000>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    LimitRequestBody 114688
    <IfVersion >= 2.4>
      ErrorLogFormat "%{cu}t %M"
    </IfVersion>
    ErrorLog /var/log/httpd/keystone.log
    CustomLog /var/log/httpd/keystone_access.log combined

    <Directory /usr/bin>
        <IfVersion >= 2.4>
            Require all granted
        </IfVersion>
        <IfVersion < 2.4>
            Order allow,deny
            Allow from all
        </IfVersion>
    </Directory>
</VirtualHost>

Alias /identity /usr/bin/keystone-wsgi-public
<Location /identity>
    SetHandler wsgi-script
    Options +ExecCGI

    WSGIProcessGroup keystone-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
</Location>
EOF
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
chown -R keystone:keystone /etc/keystone

# Các lệnh dưới đây sẽ chỉ thực hiện trên controller1
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone 
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone


# Thực hiện các lệnh dưới đây để copy key sang các con controller còn lại, để thự hiện bootstrap với VIP luôn, mà không phải lòng vòng như các bước phía bên dưới đây.

# for node in controller2 controller3
# do
# scp -rp /etc/keystone/credential-keys root@$node:/etc/keystone/
# scp -rp /etc/keystone/fernet-keys root@$node:/etc/keystone/
# ssh root@$node "chown -R keystone:keystone /etc/keystone/fernet-keys/"
# ssh root@$node "chown -R keystone:keystone /etc/keystone/credential-keys/"
# done

# keystone-manage bootstrap --bootstrap-password admin_123 \
#   --bootstrap-admin-url http://controller:5000/v3/ \
#   --bootstrap-internal-url http://controller:5000/v3/ \
#   --bootstrap-public-url http://controller:5000/v3/ \
#   --bootstrap-region-id RegionOne


keystone-manage bootstrap --bootstrap-password trang1234 \
  --bootstrap-admin-url http://192.168.40.71:5000/v3/ \
  --bootstrap-internal-url http://192.168.40.71:5000/v3/ \
  --bootstrap-public-url http://192.168.40.71:5000/v3/ \
  --bootstrap-region-id RegionOne

## Tạo endpoint keystone cho vip 
openstack endpoint create --region RegionOne identity public http://192.168.40.78:5000/v3
openstack endpoint create --region RegionOne identity admin http://192.168.40.78:5000/v3
openstack endpoint create --region RegionOne identity internal http://192.168.40.78:5000/v3
```

* Cấu hình http, chú ý đổi theo từng controller

```sh
cp /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.origin
cat <<EOF >>/etc/httpd/conf/httpd.conf
ServerName controller3
EOF
sed -i 's/Listen 80/Listen 192.168.40.77:80/g' /etc/httpd/conf/httpd.conf

```

* Hoàn tất cài đặt keystone

```sh
systemctl start httpd 
systemctl enable httpd 

cat <<EOF > ~/keystonerc
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=trang1234
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export PS1='[\u@\h \W(openstack)]\$ '
export OS_AUTH_TYPE=password
EOF

chmod 600 ~/keystonerc 
source ~/keystonerc 
echo "source ~/keystonerc " >> ~/.bash_profile
```

* Xoa cac endpoint cua controller 1 di (ip là 192.168.40.71)

```sh
[root@controller1 ~(openstack)]$ openstack endpoint list
+----------------------------------+-----------+--------------+--------------+---------+-----------+-------------------------------+
| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                           |
+----------------------------------+-----------+--------------+--------------+---------+-----------+-------------------------------+
| 16c09761f05040c9b7ceb8db172af329 | RegionOne | keystone     | identity     | True    | internal  | http://192.168.40.78:5000/v3  |
| 2a4afee4c2f64819aaa7b891a6185127 | RegionOne | keystone     | identity     | True    | admin     | http://192.168.40.78:5000/v3  |
| 6d5a9016755a43b4ab2b62a564f78dec | RegionOne | keystone     | identity     | True    | internal  | http://192.168.40.71:5000/v3/ |
| 86b46cce89834674ad7bbe345f38ebd4 | RegionOne | keystone     | identity     | True    | public    | http://192.168.40.71:5000/v3/ |
| 8e90de53a55242da816e01098ad2052b | RegionOne | keystone     | identity     | True    | public    | http://192.168.40.78:5000/v3  |
| a893241ac0754eac899d8fa4e1cd8169 | RegionOne | keystone     | identity     | True    | admin     | http://192.168.40.71:5000/v3/ |
+----------------------------------+-----------+--------------+--------------+---------+-----------+-------------------------------+
[root@controller1 ~(openstack)]$ openstack endpoint delete 6d5a9016755a43b4ab2b62a564f78dec 86b46cce89834674ad7bbe345f38ebd4 a893241ac0754eac899d8fa4e1cd8169
[root@controller1 ~(openstack)]$ openstack endpoint list
+----------------------------------+-----------+--------------+--------------+---------+-----------+------------------------------+
| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                          |
+----------------------------------+-----------+--------------+--------------+---------+-----------+------------------------------+
| 16c09761f05040c9b7ceb8db172af329 | RegionOne | keystone     | identity     | True    | internal  | http://192.168.40.78:5000/v3 |
| 2a4afee4c2f64819aaa7b891a6185127 | RegionOne | keystone     | identity     | True    | admin     | http://192.168.40.78:5000/v3 |
| 8e90de53a55242da816e01098ad2052b | RegionOne | keystone     | identity     | True    | public    | http://192.168.40.78:5000/v3 |
+----------------------------------+-----------+--------------+--------------+---------+-----------+------------------------------+
```

* Cấu hình pacemaker + haproxy

```sh
yum install pacemaker pcs resource-agents -y 
systemctl start pcsd.service
systemctl enable pcsd.service

echo "hacluster:trang1234" | chpasswd
# Khởi tạo cluster trên một node controller1
pcs cluster auth controller1 controller2 controller3 -u hacluster -p trang1234 --force
pcs cluster setup --force --name hacluster controller1 controller2 controller3

pcs cluster start --all
pcs cluster enable --all

corosync-cfgtool -s
pcs status corosync

# Cấu hình HAproxy trên cả 3 node

yum install haproxy -y
cp -np /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.org

## Chú ý IP
cat <<EOF> /etc/haproxy/haproxy.cfg
global
    chroot  /var/lib/haproxy
    daemon
    group  haproxy
    maxconn  4000
    pidfile  /var/run/haproxy.pid
    user  haproxy
 
defaults
    log  global
    maxconn  4096
    option  redispatch
    retries  3
    mode    tcp
    timeout  http-request 60s
    timeout  queue 1m
    timeout  connect 60s
    timeout  client 1m
    timeout  server 1m
    timeout  check 60s
  
listen stats 
    bind 192.168.40.78:9000
    mode http
    stats enable
    stats uri /stats
    stats show-legends
    stats realm HAProxy\ Statistics
    stats auth admin:123@123Aa
    stats admin if TRUE
 
listen dashboard_cluster
    bind 192.168.40.78:80
    balance  roundrobin
    mode http
    option  httpchk
    option  tcplog
    cookie  SERVERID insert indirect nocache
    server controller1 192.168.40.71:80 cookie controller1 check inter 2000 rise 2 fall 5 
    server controller2 192.168.40.76:80 cookie controller2 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:80 cookie controller3 check inter 2000 rise 2 fall 5 

listen mariadb_cluster 
    bind 192.168.40.78:3306
    mode tcp
    balance roundrobin
    option httpchk GET /
    option tcpka
    option httpchk
    timeout client  28800s
    timeout server  28800s  
    stick-table type ip size 1000
    stick on dst
    server controller1 192.168.40.71:3306 check port 9200 inter 5s fastinter 2s rise 3 fall 3
    server controller2 192.168.40.76:3306 check port 9200 inter 5s fastinter 2s rise 3 fall 3 
    server controller3 192.168.40.77:3306 check port 9200 inter 5s fastinter 2s rise 3 fall 3 
 
listen keystone_public_internal_cluster
    bind 192.168.40.78:5000
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.40.71:5000 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:5000 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:5000 check inter 2000 rise 2 fall 5 
 
listen glance_api_cluster
    bind 192.168.40.78:9292
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.40.71:9292 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:9292 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:9292 check inter 2000 rise 2 fall 5 
 
listen glance_registry_cluster
    bind 192.168.40.78:9191
    balance  roundrobin
    option  tcpka
    option  tcplog
    server controller1 192.168.40.71:9191 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:9191 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:9191 check inter 2000 rise 2 fall 5 
 
listen nova_compute_api_cluster
    bind 192.168.40.78:8774
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.40.71:8774 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:8774 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:8774 check inter 2000 rise 2 fall 5 
 
listen nova_vncproxy_cluster
    bind 192.168.40.78:6080
    balance  roundrobin
    option  tcpka
    option  tcplog
    capture request header X-Auth-Project-Id len 50
    capture request header User-Agent len 50
    server controller1 192.168.40.71:6080 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:6080 check inter 2000 rise 2 fall 5
    server controller3 192.168.40.77:6080 check inter 2000 rise 2 fall 5
 
listen nova_metadata_api_cluster
  bind  192.168.40.78:8775
  balance  roundrobin
  option  tcpka
  option  tcplog
  server controller1 192.168.40.71:8775 check inter 2000 rise 2 fall 5
  server controller2 192.168.40.76:8775 check inter 2000 rise 2 fall 5
  server controller3 192.168.40.77:8775 check inter 2000 rise 2 fall 5

listen nova_placement_api
    bind 192.168.40.78:8778
    balance roundrobin
    option tcpka
    option tcplog
    http-request del-header X-Forwarded-Proto
    server controller1 192.168.40.71:8778 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:8778 check inter 2000 rise 2 fall 5
    server controller3 192.168.40.77:8778 check inter 2000 rise 2 fall 5    

listen neutron_api_cluster
    bind 192.168.40.78:9696
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.40.71:9696 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:9696 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:9696 check inter 2000 rise 2 fall 5 

listen cinder_api_cluster
    bind 192.168.40.78:8776
    balance  roundrobin
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 192.168.40.71:8776 check inter 2000 rise 2 fall 5
    server controller2 192.168.40.76:8776 check inter 2000 rise 2 fall 5 
    server controller3 192.168.40.77:8776 check inter 2000 rise 2 fall 5 
EOF

systemctl stop httpd
# Stop mariadb lần lượt theo thứ tự từ controller 3 -> 1
systemctl stop mariadb 

# Khởi tạo các resource trên node Controller1
pcs property set stonith-enabled=false --force
pcs resource create VIP ocf:heartbeat:IPaddr2 ip=192.168.40.78 cidr_netmask=32  op monitor interval=30s
pcs resource create HAproxy systemd:haproxy op monitor interval=2s
pcs constraint colocation add VIP with HAproxy INFINITY
pcs resource move VIP controller1

pcs property set no-quorum-policy=ignore

pcs status
pcs resource
pcs resource show VIP

pcs resource restart HAproxy
curl 192.168.40.78
pcs resource

# Khởi động lại các service theo thứ tự từ controller 1 -> 3
systemctl start httpd

systemctl start mariadb

# Tạo một project 
openstack project create --domain default --description "Service Project" service
```

* Cài đặt glance

```sh
openstack user create --domain default --project service --password trang1234 glance
openstack role add --project service --user glance admin
openstack service create --name glance --description "OpenStack Image service" image
openstack endpoint create --region RegionOne image public http://controller:9292
openstack endpoint create --region RegionOne image internal http://controller:9292
openstack endpoint create --region RegionOne image admin http://controller:9292

mysql -u root -ptrang1234
create database glance;
grant all privileges on glance.* to glance@'localhost' identified by 'trang1234';
grant all privileges on glance.* to glance@'%' identified by 'trang1234';
flush privileges;
exit


# Thực hiện cài đặt trên cả ba node
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-glance
cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.org 

cat <<EOF > /etc/glance/glance-api.conf
[DEFAULT]
vip = 192.168.40.78
bind_host = 192.168.40.71

[glance_store]
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[database]
# MariaDB connection info
connection = mysql+pymysql://glance:trang1234@controller/glance

# keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211,controller2:11211,controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = trang1234

[paste_deploy]
flavor = keystone
EOF

cp /etc/glance/glance-registry.conf /etc/glance/glance-registry.conf.bka
cat <<EOF >/etc/glance/glance-registry.conf
[DEFAULT]
bind_host = 192.168.40.71

[database]
# MariaDB connection info
connection = mysql+pymysql://glance:trang1234@192.168.40.78/glance

# keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211,controller2:11211,controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = trang1234

[paste_deploy]
flavor = keystone
EOF

su -s /bin/bash glance -c "glance-manage db_sync" 
systemctl start openstack-glance-api openstack-glance-registry 
systemctl enable openstack-glance-api openstack-glance-registry 
```

* Install Compute service 

```sh
mysql -u root -ptrang1234
CREATE DATABASE nova_api;
CREATE DATABASE nova;
CREATE DATABASE nova_cell0;
CREATE DATABASE placement;
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' IDENTIFIED BY 'trang1234';
flush privileges;
exit

# Tạo user, service, endpoint chỉ thực hiện trên một node controller1
openstack user create --domain default --project service --password trang1234 nova
openstack role add --project service --user nova admin
openstack service create --name nova --description "OpenStack Compute" compute
openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1
openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1
openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1

openstack user create --domain default --project service --password trang1234 placement
openstack role add --project service --user placement admin
openstack service create --name placement --description "Placement API" placement
openstack endpoint create --region RegionOne placement internal http://controller:8778
openstack endpoint create --region RegionOne placement admin http://controller:8778


# Cài đặt các thành phần của nova trên cả ba node
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-nova-api openstack-nova-conductor \
  openstack-nova-console openstack-nova-novncproxy \
  openstack-nova-scheduler openstack-nova-placement-api


cp /etc/nova/nova.conf /etc/nova/nova.conf.org
cat <<EOF >/etc/nova/nova.conf 
[DEFAULT]
debug = true
state_path = /var/lib/nova
enabled_apis = osapi_compute,metadata
log_dir = /var/log/nova
transport_url = rabbit://openstack:trang1234@192.168.40.71,openstack:trang1234@192.168.40.76,openstack:trang1234@192.168.40.77
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver
osapi_compute_listen=192.168.40.77
metadata_listen=192.168.40.77
metadata_host = 192.168.40.77
rpc_response_timeout = 180

[api]
auth_strategy = keystone
[api_database]
connection = mysql+pymysql://nova:trang1234@controller/nova_api

[database]
connection = mysql+pymysql://nova:trang1234@controller/nova
[devices]
[ephemeral_storage_encryption]
[filter_scheduler]
[glance]
api_servers = http://controller:9292

[keystone_authtoken]
auth_url = http://controller:5000
memcached_servers = controller1:11211,controller2:11211,controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = Default
user_domain_name = Default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234
service_metadata_proxy = True
metadata_proxy_shared_secret = trang1234

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[placement]
auth_url = http://controller:5000
os_region_name = RegionOne
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[placement_database]
connection = mysql+pymysql://placement:trang1234@controller/placement
[vnc]
enabled = true
server_listen = 0.0.0.0
server_proxyclient_address = 192.168.40.77
novncproxy_host = 192.168.40.77

[wsgi]
api_paste_config = /etc/nova/api-paste.ini

[oslo_messaging_rabbit]
rabbit_retry_interval=1
rabbit_retry_backoff=2
rabbit_max_retries=0
rabbit_ha_queues= true
EOF


cp /etc/httpd/conf.d/00-nova-placement-api.conf /etc/httpd/conf.d/00-nova-placement-api.conf.org
cat <<EOF >/etc/httpd/conf.d/00-nova-placement-api.conf
Listen 192.168.40.77:8778

<VirtualHost 192.168.40.77:8778>
  WSGIProcessGroup nova-placement-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIPassAuthorization On
  WSGIDaemonProcess nova-placement-api processes=3 threads=1 user=nova group=nova
  WSGIScriptAlias / /usr/bin/nova-placement-api
  <IfVersion >= 2.4>
    ErrorLogFormat "%M"
  </IfVersion>
  ErrorLog /var/log/nova/nova-placement-api.log
  #SSLEngine On
  #SSLCertificateFile ...
  #SSLCertificateKeyFile ...
</VirtualHost>

Alias /nova-placement-api /usr/bin/nova-placement-api
<Location /nova-placement-api>
  SetHandler wsgi-script
  Options +ExecCGI
  WSGIProcessGroup nova-placement-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIPassAuthorization On
</Location>

<Directory /usr/bin>
 <IfVersion >= 2.4>
    Require all granted
 </IfVersion>
 <IfVersion < 2.4>
    Order allow,deny
    Allow from all
 </IfVersion>
</Directory>
EOF

# 3 command dưới đây thực hiện chạy trên tất cả các node compute (thử lại thì chỉ cần làm trên một controller1)
su -s /bin/sh -c "nova-manage api_db sync" nova
su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
su -s /bin/sh -c "nova-manage db sync" nova

# command dưới nay chi được dùng trên một node thôi
su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova 

su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova

chown nova:nova /var/log/nova/nova-placement-api.log

systemctl enable openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service
systemctl start openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service

systemctl restart openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service
```

## Trên hai node compute

* Cài đặt và cấu hình  nova compute service 
```sh
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-nova-compute openstack-selinux

cp /etc/nova/nova.conf /etc/nova/nova.conf.orig 
cat <<EOF > /etc/nova/nova.conf
[DEFAULT]
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3
my_ip = 192.168.40.72
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver
instance_usage_audit = True
instance_usage_audit_period = hour
notify_on_state_change = vm_and_task_state
rpc_response_timeout = 180

[api]
auth_strategy = keystone

[glance]
api_servers = http://controller:9292

[keystone_authtoken]
auth_url = http://controller:5000/v3
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = Default
user_domain_name = Default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[oslo_messaging_notifications]
driver = messagingv2

[placement]
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller:5000/v3
username = placement
password = trang1234

[vnc]
enabled = true
server_listen = 0.0.0.0
server_proxyclient_address = \$my_ip
novncproxy_base_url = http://controller:6080/vnc_auto.html

[oslo_messaging_rabbit]
rabbit_retry_interval=1
rabbit_retry_backoff=2
rabbit_max_retries=0
rabbit_ha_queues= true

[libvirt]
virt_type = kvm
EOF

modprobe br_netfilter
cat <<EOF > /etc/sysctl.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl -p

systemctl enable libvirtd.service openstack-nova-compute.service
systemctl start libvirtd.service openstack-nova-compute.service

# Add node trên controller1
openstack compute service list --service nova-compute
su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
```

## Trên 3 node controller

* Cài đặt Neutron

```sh
# Thực hiện trên một node controller 1
mysql -u root -ptrang1234
CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'trang1234';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'trang1234';
flush privileges;
exit

openstack user create --domain default --project service --password trang1234 neutron
openstack role add --project service --user neutron admin
openstack service create --name neutron --description "OpenStack Networking" network
openstack endpoint create --region RegionOne network public http://controller:9696
openstack endpoint create --region RegionOne network internal http://controller:9696
openstack endpoint create --region RegionOne network admin http://controller:9696

# Cài đặt trên cả ba node
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables
## neu loi thiếu thư viện khi khởi động openvswitch thì cài đặt gọi sau và khởi động lại các service network, openvswitch, neutron-openvswitch-agent
yum install libibverbs -y 

cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.org

cat<<EOF > /etc/neutron/neutron.conf
[DEFAULT]
bind_host = 192.168.40.77
auth_strategy = keystone
core_plugin = ml2
service_plugins = router
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
rpc_response_timeout = 180
dhcp_agents_per_network = 3
l3_ha = true
max_l3_agents_per_router = 3

[database]
connection = mysql+pymysql://neutron:trang1234@controller/neutron

[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211,controller2:11211,controller3:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = neutron
password = trang1234

[nova]
auth_url = http://controller:5000
auth_type = password
project_domain_name = Default
user_domain_name = Default
region_name = RegionOne
project_name = service
username = nova
password = trang1234

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp

[oslo_messaging_rabbit]
rabbit_retry_interval=1
rabbit_retry_backoff=2
rabbit_max_retries=0
rabbit_ha_queues= true
[oslo_concurrency]

EOF

cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.org
cat <<EOF > /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
extension_drivers = port_security

[ml2_type_flat]
flat_networks = provider

[ml2_type_vlan]
network_vlan_ranges = provider

[ml2_type_vxlan]
vni_ranges = 1:1000

[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
enable_ipset = True
EOF

cp /etc/neutron/plugins/ml2/openvswitch_agent.ini /etc/neutron/plugins/ml2/openvswitch_agent.ini.bka
cat <<EOF > /etc/neutron/plugins/ml2/openvswitch_agent.ini 
[DEFAULT]
[agent]
tunnel_types = vxlan
l2_population = True
[network_log]
[ovs]
bridge_mappings = provider:br-provider
local_ip = 192.168.50.77
[securitygroup]
enable_security_group = True
firewall_driver = iptables_hybrid
[xenapi]
EOF

cp /etc/neutron/dhcp_agent.ini /etc/neutron/dhcp_agent.ini.bka
cat <<EOF > /etc/neutron/dhcp_agent.ini 
[DEFAULT]
interface_driver = openvswitch
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = True
force_metadata = True
EOF

cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.bka
cat <<EOF > /etc/neutron/metadata_agent.ini
[default]
nova_metadata_host = 192.168.40.71
nova_metadata_ip = 192.168.40.78
metadata_proxy_shared_secret = trang1234
memcached_servers = controller1:11211,controller2:11211,controller3:11211
EOF
```

* Cấu hình card mạng trên cả 3 node controller tương tự như sau:

```sh
cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=no
IPADDR="192.168.50.73"
PREFIX="24"
IPV4_FAILURE_FATAL=no
NAME=eth1
DEVICE=eth1
ONBOOT=yes
EOF

cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth2
DEVICE=eth2
NAME=eth2
DEVICETYPE=ovs
TYPE=OVSPort
OVS_BRIDGE=br-provider
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
EOF

cat  <<EOF > /etc/sysconfig/network-scripts/ifcfg-br-provider
ONBOOT=yes
IPADDR=192.168.68.73
NETMASK=255.255.255.0
DEVICE=br-provider
NAME=br-provider
DEVICETYPE=ovs
OVSBOOTPROTO=none
TYPE=OVSBridge
DEFROUTE=no
EOF

cp /etc/neutron/l3_agent.ini /etc/neutron/l3_agent.ini.bka
cat <<EOF > /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = openvswitch
external_network_bridge =

[agent]
[ovs]
EOF
```

* Khởi động lại các service 

```sh
systemctl restart network openvswitch
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini

su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

systemctl restart openstack-nova-api.service
systemctl enable neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
systemctl restart neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
systemctl enable neutron-l3-agent.service
systemctl restart neutron-l3-agent.service

systemctl restart openvswitch.service neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent
```

## Trên hai node compute

* Cài đặt neutron openvswitch agent

```sh
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-neutron-openvswitch ebtables ipset
cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.org

cat <<EOF > /etc/neutron/neutron.conf
[DEFAULT]
core_plugin = ml2
transport_url = rabbit://openstack:trang1234@controller1,openstack:trang1234@controller2,openstack:trang1234@controller3
auth_strategy = keystone
[agent]
[cors]
[database]
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211,controller2:11211,controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = trang1234
[matchmaker_redis]
[nova]
[oslo_concurrency]
lock_path = /var/lib/neutron/tmp

[oslo_messaging_rabbit]
rabbit_retry_interval=1
rabbit_retry_backoff=2
rabbit_max_retries=0
rabbit_ha_queues= true
EOF

cat <<EOF > /etc/neutron/plugins/ml2/openvswitch_agent.ini
[DEFAULT]
[agent]
tunnel_types = vxlan
l2_population = True
[network_log]
[ovs]
bridge_mappings = provider:br-provider
local_ip = 192.168.50.73
[securitygroup]
enable_security_group = True
firewall_driver = iptables_hybrid
[xenapi]
EOF

systemctl restart network
systemctl restart openvswitch.service
systemctl restart neutron-openvswitch-agent.service
```



* Cài đặt Horizon

```sh
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-dashboard
cp /etc/openstack-dashboard/local_settings /etc/openstack-dashboard/local_settings.origin
vim /etc/openstack-dashboard/local_settings

# Chỉnh sửa file: vim  /etc/openstack-dashboard/local_settings

OPENSTACK_HOST = "192.168.40.78"

ALLOWED_HOSTS = ['*', ]

# SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': ['controller1:11211','controller2:11211','controller3:11211', ], 
    },
}

OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 2,
}

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"

OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

#If you chose networking option 2
OPENSTACK_NEUTRON_NETWORK = {
    #...
    'enable_router': True,
    'enable_quotas': True,
    'enable_ipv6': False,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_fip_topology_check': True,
}
```

* Thêm dòng sau vào file `/etc/httpd/conf.d/openstack-dashboard.conf`

```sh
WSGIApplicationGroup %{GLOBAL}
```

* Khởi động lại service

```sh
systemctl restart httpd.service memcached.service
```


## Trên controller 1

Tạo các tài nguyên cần thiết rồi tạo máy ảo để test

```sh
# Tạo network 
openstack network create  --share --external --provider-physical-network provider --provider-network-type flat provider

openstack subnet create --network provider \
  --allocation-pool start=192.168.68.81,end=192.168.68.100 \
  --dns-nameserver 8.8.8.8 --gateway 192.168.68.1 \
  --subnet-range 192.168.68.0/24 provider

openstack network create selfservice

openstack subnet create --network selfservice \
  --dns-nameserver 8.8.8.8 --gateway 10.10.10.1 \
  --subnet-range 10.10.10.0/24 selfservice

openstack router create router
neutron router-interface-add router selfservice
neutron router-gateway-set router provider

ip netns
neutron router-port-list router


# Tạo flavor
openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano


# Up image
wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
openstack image create "cirros" \
  --file cirros-0.3.4-x86_64-disk.img \
  --disk-format qcow2 --container-format bare \
  --public
```

