# Cài đặt và cấu hình HA cho controller node

## 1. Chuẩn bị môi trường 

Gồm 3 node controller:

* Controller 1

```sh
eth0: 192.168.40.71  - PROVIDER
eth1: 192.168.50.71  - DATAVM
eth2: 192.168.68.71  - MANAGERMENT
```

* Controller 2

```sh
eth0: 192.168.40.76
eth1: 192.168.50.76
eth2: 192.168.68.76
```

* Controller 3

```sh
eth0: 192.168.40.77
eth1: 192.168.50.77
eth2: 192.168.68.77
```

## 2. Cài đặt trên ba node

Controller 1 đang chạy và chuẩn bị add thêm 2 node controller nữa



### Trên Controller 2 (Controller 3 tương tự, nhớ phải sửa đổi ip trên một số chỗ)

```sh
hostnamectl set-hostname controller2
systemctl disable firewalld
systemctl stop firewalld
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

yum install chrony -y
sed -i "s/server 0.centos.pool.ntp.org iburst/server 192.168.40.71 iburst/g" /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/#allow 192.168.0.0\/16/allow 192.168.40.0\/24/g' /etc/chrony.conf

systemctl enable chronyd.service
systemctl start chronyd.service

chronyc sources

yum -y install centos-release-openstack-rocky 
sed -i -e "s/enabled=1/enabled=0/g" /etc/yum.repos.d/CentOS-OpenStack-rocky.repo 


# MariaDB server 

yum --enablerepo=centos-openstack-rocky -y install mariadb-server

cat <<EOF > /etc/my.cnf.d/openstack.cnf
[mysqld]

bind-address = 192.168.40.71
default-storage-engine = innodb
innodb_file_per_table
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8
EOF

systemctl enable mariadb.service
systemctl start mariadb.service

# RabbitMQ

yum --enablerepo=epel -y install rabbitmq-server memcached
systemctl start rabbitmq-server memcached 
systemctl enable rabbitmq-server memcached

sed -i 's/OPTIONS=\"-l 127.0.0.1,::1\"/OPTIONS=\"-l 127.0.0.1,::1,192.168.40.71\"/g' /etc/sysconfig/memcached
systemctl restart rabbitmq-server memcached

```

### Trên Controller 1

```sh
rabbitmq-plugins enable rabbitmq_management  
rabbitmq-server -detached  

# Kiểm tra status 
rabbitmqctl cluster_status
```

```sh
[root@controller ~(openstack)]# rabbitmqctl cluster_status
Cluster status of node rabbit@controller ...
[{nodes,[{disc,[rabbit@controller]}]},
 {running_nodes,[rabbit@controller]},
 {cluster_name,<<"rabbit@controller">>},
 {partitions,[]}]
...done.
```

Copy file cookie từ master node (controller 1) sang hai node slave (controller 2 + controller 3) còn lại 

```sh
scp  /var/lib/rabbitmq/.erlang.cookie root@192.168.40.76:/var/lib/rabbitmq/
scp  /var/lib/rabbitmq/.erlang.cookie root@192.168.40.77:/var/lib/rabbitmq/

# add node vào cluster
sudo systemctl restart rabbitmq-server
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@controller3
rabbitmqctl start_app
rabbitmqctl cluster_status
```

### Trên node Controller 2

```sh
sudo systemctl restart rabbitmq-server
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@controller
rabbitmqctl start_app
rabbitmqctl cluster_status
```

Kết quả kiểm tra 

```sh
[root@controller ~(openstack)]# rabbitmqctl cluster_status
Cluster status of node rabbit@controller ...
[{nodes,[{disc,[rabbit@controller,rabbit@controller2,rabbit@controller3]}]},
 {running_nodes,[rabbit@controller2,rabbit@controller3,rabbit@controller]},
 {cluster_name,<<"rabbit@controller3">>},
 {partitions,[]}]
...done.
```

```sh
rabbitmqctl add_user openstack trang1234
rabbitmqctl set_permissions openstack ".*" ".*" ".*"
```


### Cấu hình trên 3 node Controller (đổi IP )

```sh
yum --enablerepo=centos-openstack-rocky -y install galera rsync mariadb-server-galera.x86_64

cat <<EOF > /etc/my.cnf.d/galera.cnf
[galera]
# Mandatory settings
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so

#add your node ips here
wsrep_cluster_address="gcomm://192.168.40.71,192.168.40.76,192.168.40.77"
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
#Cluster name

wsrep_cluster_name="galera_cluster"
# Allow server to accept connections on all interfaces.

bind-address=192.168.40.71

# this server ip, change for each server
wsrep_node_address="192.168.40.71"
# this server name, change for each server
wsrep_node_name="node1"

wsrep_sst_method=rsync
EOF
```

### Controller 1

```sh
galera_new_cluster
cat /var/lib/mysql/grastate.dat
systemctl restart mariadb
mysql -u root -ptrang1234 -e "SHOW STATUS LIKE 'wsrep_cluster_size'"
```



### Trên cả 3 node

```sh
# Install  Keystone
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi
```

Tiếp theo cần xóa toàn bộ endpoint cũ đi

```sh
[root@controller ~(openstack)]# openstack endpoint list
+----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------+
| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                            |
+----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------+
| 036d4784ff37455f96377a288876fc0f | RegionOne | keystone     | identity     | True    | admin     | http://192.168.40.71:5000/v3/  |
| 170ba69b4fd14fa3b85ab848880a530a | RegionOne | placement    | placement    | True    | internal  | http://192.168.40.71:8778      |
| 19f4999fceb54b38908f51ebb0dfdf8f | RegionOne | nova         | compute      | True    | public    | http://192.168.40.71:8774/v2.1 |
| 1c871b45771f419d8b4b8b9233f8b0af | RegionOne | neutron      | network      | True    | public    | http://192.168.40.71:9696      |
| 3a67a667080f47e2be6f89cfb2db692c | RegionOne | glance       | image        | True    | internal  | http://192.168.40.71:9292      |
| 5b9facf055c74a81b2d2d7d40c25401f | RegionOne | gnocchi      | metric       | True    | internal  | http://192.168.40.71:8041      |
| 63df25658e484182a8d06e17bcdac00b | RegionOne | gnocchi      | metric       | True    | admin     | http://192.168.40.71:8041      |
| 738c13fe7f224fa3b0bbe7e4c79ba949 | RegionOne | aodh         | alarming     | True    | admin     | http://192.168.40.71:8042      |
| 83f2a16b9dd3401994f98bf14aa0d959 | RegionOne | glance       | image        | True    | admin     | http://192.168.40.71:9292      |
| a7e22003494f4b26bedbba293a0015d9 | RegionOne | placement    | placement    | True    | admin     | http://192.168.40.71:8778      |
| ac1f54b5a50842f1b4ae54c74db36824 | RegionOne | nova         | compute      | True    | admin     | http://192.168.40.71:8774/v2.1 |
| ade19b8100744a65a5e31ef84c560582 | RegionOne | keystone     | identity     | True    | internal  | http://192.168.40.71:5000/v3/  |
| b4247e55f01f4e95a51139ffc289d619 | RegionOne | glance       | image        | True    | public    | http://192.168.40.71:9292      |
| be301adc987a46ec9f0371739bd9f5f8 | RegionOne | neutron      | network      | True    | internal  | http://192.168.40.71:9696      |
| d0f540c92c6f4e19afc531989975fe7b | RegionOne | gnocchi      | metric       | True    | public    | http://192.168.40.71:8041      |
| e69b5c852a0f41d8aeafea4d6e90e3dd | RegionOne | neutron      | network      | True    | admin     | http://192.168.40.71:9696      |
| e73d2a3e08ac4b0486b7803b7a82d7fb | RegionOne | nova         | compute      | True    | internal  | http://192.168.40.71:8774/v2.1 |
| e8916bd39fe74b07bf4a8dd2a90c04fe | RegionOne | aodh         | alarming     | True    | public    | http://192.168.40.71:8042      |
| f52601ec390e4aee87e688e09d85c2a2 | RegionOne | aodh         | alarming     | True    | internal  | http://192.168.40.71:8042      |
| fb1ffba3acfc4ab7a12e95047dc8da03 | RegionOne | keystone     | identity     | True    | public    | http://192.168.40.71:5000/v3/  |
+----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------+

# Tao tat ca cac endpoint can thiet

## Tạo endpoint ban đầu trên controller 
su -s /bin/bash keystone -c "keystone-manage db_sync"

keystone-manage bootstrap --bootstrap-password trang1234 \
  --bootstrap-admin-url http://192.168.40.71:5000/v3/ \
  --bootstrap-internal-url http://192.168.40.71:5000/v3/ \
  --bootstrap-public-url http://192.168.40.71:5000/v3/ \
  --bootstrap-region-id RegionOne

## Tạo endpoint keystone cho vip 
openstack endpoint create --region RegionOne identity public http://192.168.40.78:5000/v3
openstack endpoint create --region RegionOne identity admin http://192.168.40.78:5000/v3
openstack endpoint create --region RegionOne identity internal http://192.168.40.78:5000/v3

[root@controller3 ~(openstack)]# cat keystonerc
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=trang1234
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export PS1='[\u@\h \W(openstack)]\$ '
export OS_AUTH_TYPE=password


## xóa các endpoint keystone cũ đi


## 
openstack endpoint create --region RegionOne image public http://192.168.40.78:9292
openstack endpoint create --region RegionOne image internal http://192.168.40.78:9292
openstack endpoint create --region RegionOne image admin http://192.168.40.78:9292

openstack endpoint create --region RegionOne compute public http://192.168.40.78:8774/v2.1
openstack endpoint create --region RegionOne compute internal http://192.168.40.78:8774/v2.1
openstack endpoint create --region RegionOne compute admin http://192.168.40.78:8774/v2.1

openstack endpoint create --region RegionOne placement internal http://192.168.40.78:8778
openstack endpoint create --region RegionOne placement admin http://192.168.40.78:8778

openstack endpoint create --region RegionOne network public http://192.168.40.78:9696
openstack endpoint create --region RegionOne network internal http://192.168.40.78:9696
openstack endpoint create --region RegionOne network admin http://192.168.40.78:9696

cat /etc/hosts
127.0.0.1   controler1 localhost.localdomain localhost4 localhost4.localdomain4
::1         controller1 localhost.localdomain localhost6 localhost6.localdomain6
192.168.40.71   controller1 
192.168.40.72   compute1
192.168.40.73   compute2
192.168.40.74   storage
192.168.40.123  trang-40-123
192.168.40.76   controller2
192.168.40.77   controller3
192.168.40.78   controller
```



## Cấu hình keystone trên cả ba node
```sh
cat <<EOF > /etc/keystone/keystone.conf
[cache]
memcache_servers = 192.168.40.71:11211,192.168.40.76:11211,192.168.40.77:11211
[cors]
allowed_origin = http://192.168.40.129:3000
allow_methods = GET,PUT,POST,DELETE,PATCH
allow_headers = X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-Domain-Id,X-Domain-Name
[database]
connection = mysql+pymysql://keystone:trang1234@192.168.40.78/keystone
[token]
provider = fernet
EOF

su -s /bin/bash keystone -c "keystone-manage db_sync"
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
systemctl start httpd 
systemctl enable httpd 


# Glance trên 3 node ctl (chú ý ip)

vip='192.168.40.78'
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-glance
cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.bka 

cat <<EOF > /etc/glance/glance-api.conf
[DEFAULT]
vip = 192.168.40.78
bind_host = 192.168.40.76

[glance_store]
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[database]
# MariaDB connection info
connection = mysql+pymysql://glance:trang1234@controller/glance

# keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = trang1234

[paste_deploy]
flavor = keystone
EOF

cp /etc/glance/glance-registry.conf /etc/glance/glance-registry.conf.bka
cat <<EOF >/etc/glance/glance-registry.conf
[DEFAULT]
vip = 192.168.40.78
bind_host = 192.168.40.76

[database]
# MariaDB connection info
connection = mysql+pymysql://glance:trang1234@192.168.40.78/glance

# keystone auth info
[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller1:11211, controller2:11211, controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = trang1234

[paste_deploy]
flavor = keystone
EOF

su -s /bin/bash glance -c "glance-manage db_sync" 
systemctl start openstack-glance-api openstack-glance-registry 
systemctl enable openstack-glance-api openstack-glance-registry 


# Compute 
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-nova-api openstack-nova-conductor \
openstack-nova-console openstack-nova-novncproxy \
openstack-nova-scheduler openstack-nova-placement-api

cp /etc/nova/nova.conf /etc/nova/nova.conf.bka

cat <<EOF >/etc/nova/nova.conf 
[DEFAULT]
debug = true
state_path = /var/lib/nova
enabled_apis = osapi_compute,metadata
log_dir = /var/log/nova
transport_url = rabbit://openstack:trang1234@192.168.40.71,openstack:trang1234@192.168.40.76,openstack:trang1234@192.168.40.77
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver
osapi_compute_listen=192.168.40.77
metadata_listen=192.168.40.77

[api]
auth_strategy = keystone
[api_database]
connection = mysql+pymysql://nova:trang1234@192.168.40.78/nova_api

[database]
connection = mysql+pymysql://nova:trang1234@192.168.40.78/nova
[devices]
[ephemeral_storage_encryption]
[filter_scheduler]
[glance]
api_servers = http://192.168.40.78:9292

[keystone_authtoken]
auth_url = http://controller:5000
memcached_servers = controller1:11211,controller2:11211,controller3:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = Default
user_domain_name = Default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234
service_metadata_proxy = True
metadata_proxy_shared_secret = trang1234

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[placement]
auth_url = http://controller:5000
os_region_name = RegionOne
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = placement
password = trang1234

[placement_database]
connection = mysql+pymysql://placement:trang1234@controller/placement
[vnc]
enabled = true
server_listen = 0.0.0.0
server_proxyclient_address = 192.168.40.77
novncproxy_host = 192.168.40.77

[wsgi]
api_paste_config = /etc/nova/api-paste.ini
EOF

cat <<EOF >/etc/httpd/conf.d/00-nova-placement-api.conf
Listen 192.168.40.71:8778

<VirtualHost 192.168.40.71:8778>
  WSGIProcessGroup nova-placement-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIPassAuthorization On
  WSGIDaemonProcess nova-placement-api processes=3 threads=1 user=nova group=nova
  WSGIScriptAlias / /usr/bin/nova-placement-api
  <IfVersion >= 2.4>
    ErrorLogFormat "%M"
  </IfVersion>
  ErrorLog /var/log/nova/nova-placement-api.log
  #SSLEngine On
  #SSLCertificateFile ...
  #SSLCertificateKeyFile ...
</VirtualHost>

Alias /nova-placement-api /usr/bin/nova-placement-api
<Location /nova-placement-api>
  SetHandler wsgi-script
  Options +ExecCGI
  WSGIProcessGroup nova-placement-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIPassAuthorization On
</Location>

<Directory /usr/bin>
 <IfVersion >= 2.4>
    Require all granted
 </IfVersion>
 <IfVersion < 2.4>
    Order allow,deny
    Allow from all
 </IfVersion>
</Directory>
EOF

# 3 command dưới đây thực hiện chạy trên tất cả các node compute
su -s /bin/sh -c "nova-manage api_db sync" nova
su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
su -s /bin/sh -c "nova-manage db sync" nova

# 1 command dưới nay chi được dùng trên một node thôi
su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova 
su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova

chown nova:nova /var/log/nova/nova-placement-api.log

systemctl enable openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service
systemctl start openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service
systemctl restart openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service
systemctl restart httpd

# Neutron
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables
cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.bka

cat<<EOF > /etc/neutron/neutron.conf
[DEFAULT]
bind_host = 192.168.40.77
auth_strategy = keystone
core_plugin = ml2
service_plugins =
transport_url = rabbit://openstack:trang1234@192.168.40.71,openstack:trang1234@192.168.40.76,openstack:trang1234@192.168.40.77
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True

[database]
connection = mysql+pymysql://neutron:trang1234@192.168.40.78/neutron

[keystone_authtoken]
auth_uri = http://192.168.40.78:5000
auth_url = http://192.168.40.78:5000
memcached_servers = 192.168.40.71:11211,192.168.40.76:11211,192.168.40.77:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = neutron
password = trang1234

[nova]
auth_url = http://192.168.40.78:5000
auth_type = password
project_domain_name = Default
user_domain_name = Default
region_name = RegionOne
project_name = service
username = nova
password = trang1234

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp
EOF

cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.bka
cat <<EOF > /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
extension_drivers = port_security
[ml2_type_flat]
flat_networks = provider

[ml2_type_vlan]
network_vlan_ranges = provider

[ml2_type_vxlan]
vni_ranges = 1:1000

[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
enable_ipset = True
EOF

cp /etc/neutron/plugins/ml2/openvswitch_agent.ini /etc/neutron/plugins/ml2/openvswitch_agent.ini.bka
cat <<EOF > /etc/neutron/plugins/ml2/openvswitch_agent.ini 
[DEFAULT]
[agent]
tunnel_types = vxlan
l2_population = True
[network_log]
[ovs]
bridge_mappings = provider:br-provider
local_ip = 192.168.50.77
[securitygroup]
enable_security_group = True
firewall_driver = iptables_hybrid
[xenapi]
EOF

cp /etc/neutron/dhcp_agent.ini /etc/neutron/dhcp_agent.ini.bka
cat <<EOF > /etc/neutron/dhcp_agent.ini 
[DEFAULT]
interface_driver = openvswitch
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = True
force_metadata = True
EOF

cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.bka
cat <<EOF > /etc/neutron/metadata_agent.ini
nova_metadata_ip = 192.168.40.77
metadata_proxy_shared_secret = trang1234
memcached_servers = controller1:11211,controller2:11211,controller3:11211
EOF

cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=no
IPADDR="192.168.50.77"
PREFIX="24"
IPV4_FAILURE_FATAL=no
NAME=eth1
DEVICE=eth1
ONBOOT=yes
EOF

cat <<EOF > /etc/sysconfig/network-scripts/ifcfg-eth2
DEVICE=eth2
NAME=eth2
DEVICETYPE=ovs
TYPE=OVSPort
OVS_BRIDGE=br-provider
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
EOF

cp /etc/neutron/l3_agent.ini /etc/neutron/l3_agent.ini.bka
cat <<EOF > /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = openvswitch
external_network_bridge =
[agent]
[ovs]
EOF

cat  <<EOF > /etc/sysconfig/network-scripts/ifcfg-br-provider
ONBOOT=yes
IPADDR=192.168.68.77
NETMASK=255.255.255.0
DEVICE=br-provider
NAME=br-provider
DEVICETYPE=ovs
OVSBOOTPROTO=none
TYPE=OVSBridge
DEFROUTE=no
EOF


systemctl restart network openvswitch
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

systemctl restart openstack-nova-api.service
systemctl enable neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
systemctl restart neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
systemctl enable neutron-l3-agent.service
systemctl restart neutron-l3-agent.service

systemctl restart openvswitch.service neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent

# Horizon 
yum --enablerepo=centos-openstack-rocky,epel -y install openstack-dashboard
cp /etc/openstack-dashboard/local_settings /etc/openstack-dashboard/local_settings.bka
```

* Chinh sua noi dung file
```sh
vim  /etc/openstack-dashboard/local_settings

OPENSTACK_HOST = "192.168.40.78"

ALLOWED_HOSTS = ['*', ]

# SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': ['192.168.40.71:11211','192.168.40.76:11211','192.168.40.77:11211', ], 
    },
}

OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 2,
}

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"

OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

#If you chose networking option 1, disable support for layer-3 networking services:
OPENSTACK_NEUTRON_NETWORK = {
    #...
    'enable_router': False,
    'enable_quotas': False,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_lb': False,
    'enable_firewall': False,
    'enable_vpn': False,
    'enable_fip_topology_check': False,
}

#If you chose networking option 2
OPENSTACK_NEUTRON_NETWORK = {
    #...
    'enable_router': True,
    'enable_quotas': True,
    'enable_ipv6': True,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_fip_topology_check': True,
}
```


Một số các câu lệnh khởi động lại các service

```sh
systemctl restart httpd

systemctl restart openstack-glance-api openstack-glance-registry 

systemctl restart openstack-nova-api.service \
  openstack-nova-scheduler.service openstack-nova-conductor.service \
  openstack-nova-novncproxy.service

systemctl restart neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service
```

**Chú ý** cần chỉnh sửa một số file như sau trên cả 3 con :

Chỉnh sửa cấu hình file  `/etc/httpd/conf/httpd.conf`

```sh
Listen 192.168.40.76:80
```

* File `/etc/httpd/conf.d/wsgi-keystone.conf`

```sh
Listen 192.168.40.76:5000

<VirtualHost 192.168.40.76:5000>
...
```

* File `/etc/httpd/conf.d/00-nova-placement-api.conf`

```sh
Listen 192.168.40.76:8778

<VirtualHost 192.168.40.76:8778>
...
```


Thêm dòng sau vào file `/etc/httpd/conf.d/openstack-dashboard.conf`

```sh
WSGIApplicationGroup %{GLOBAL}
```

Khởi động lại service 

```sh
systemctl restart httpd.service memcached.service
```




## Trên cả 3 node controller

```sh
# Pacemaker

yum install pacemaker pcs resource-agents -y 
systemctl start pcsd.service
systemctl enable pcsd.service

echo "hacluster:trang1234" | chpasswd

# Khởi tạo cluster trên một node

pcs cluster auth controller1 controller2 controller3 -u hacluster -p trang1234 --force
pcs cluster setup --force --name hacluster controller1 controller2 controller3

pcs cluster start --all
pcs cluster enable --all

corosync-cfgtool -s
pcs status corosync

pcs property set stonith-enabled=false --force
pcs resource create VIP ocf:heartbeat:IPaddr2 ip=192.168.40.78 cidr_netmask=32  op monitor interval=30s
pcs resource create HAproxy systemd:haproxy op monitor interval=2s
pcs constraint colocation add VIP with HAproxy INFINITY

pcs status
pcs resource
pcs resource show VIP

# Cấu hình HAproxy trên cả 3 node

yum install haproxy -y
cp -np /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak

## Chú ý IP
cat <<EOF >> /etc/haproxy/haproxy.cfg
listen webfarm 192.168.40.78:80 ## VIP Resource
       mode http
       stats enable
       stats auth admin:trang1234
       balance roundrobin
       option httpclose
       option forwardfor
       server controller 192.168.40.71:80 check fall 3 rise 3
       server controller2 192.168.40.76:80 check fall 3 rise 3
       server controller3 192.168.40.77:80 check fall 3 rise 3

listen webfarm 192.168.40.78:5000 ## keystone 
       mode http
       stats enable
       stats auth admin:trang1234
       balance roundrobin
       option httpclose
       option forwardfor
       server controller 192.168.40.71:5000 check fall 3 rise 3
       server controller2 192.168.40.76:5000 check fall 3 rise 3
       server controller3 192.168.40.77:5000 check fall 3 rise 3

listen webfarm 192.168.40.78:9292 
       mode http
       stats enable
       stats auth admin:trang1234
       balance roundrobin
       option httpclose
       option forwardfor
       server controller 192.168.40.71:9292 check fall 3 rise 3
       server controller2 192.168.40.76:9292 check fall 3 rise 3
       server controller3 192.168.40.77:9292 check fall 3 rise 3

listen webfarm 192.168.40.78:8774 
       mode http
       stats enable
       stats auth admin:trang1234
       balance roundrobin
       option httpclose
       option forwardfor
       server controller 192.168.40.71:8774 check fall 3 rise 3
       server controller2 192.168.40.76:8774 check fall 3 rise 3
       server controller3 192.168.40.77:8774 check fall 3 rise 3

listen webfarm 192.168.40.78:8778 
       mode http
       stats enable
       stats auth admin:trang1234
       balance roundrobin
       option httpclose
       option forwardfor
       server controller 192.168.40.71:8778 check fall 3 rise 3
       server controller2 192.168.40.76:8778 check fall 3 rise 3
       server controller3 192.168.40.77:8778 check fall 3 rise 3

listen webfarm 192.168.40.78:9696 
       mode http
       stats enable
       stats auth admin:trang1234
       balance roundrobin
       option httpclose
       option forwardfor
       server controller 192.168.40.71:9696 check fall 3 rise 3
       server controller2 192.168.40.76:9696 check fall 3 rise 3
       server controller3 192.168.40.77:9696 check fall 3 rise 3

EOF
pcs resource restart HAproxy
curl 192.168.30.145
pcs resource
```


## Trên hai con compute

```sh
cat <<EOF > /etc/nova/nova.conf
[DEFAULT]
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:trang1234@192.168.40.71,openstack:trang1234@192.168.40.76,openstack:trang1234@192.168.40.77
my_ip = 192.168.40.73
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver
instance_usage_audit = True
instance_usage_audit_period = hour
notify_on_state_change = vm_and_task_state

[api]
auth_strategy = keystone

[glance]
api_servers = http://controller:9292

[keystone_authtoken]
auth_url = http://controller:5000/v3
memcached_servers = 192.168.40.71:11211, 192.168.40.76:11211, 192.168.40.77:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = trang1234

[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = Default
user_domain_name = Default
region_name = RegionOne
project_name = service
username = neutron
password = trang1234

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[oslo_messaging_notifications]
driver = messagingv2

[placement]
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller:5000/v3
username = placement
password = trang1234

[vnc]
enabled = true
server_listen = 0.0.0.0
server_proxyclient_address = $my_ip
novncproxy_base_url = http://controller:6080/vnc_auto.html
EOF


cat <<EOF > /etc/neutron/neutron.conf
[DEFAULT]
core_plugin = ml2
transport_url = rabbit://openstack:trang1234@192.168.40.71,openstack:trang1234@192.168.40.76,openstack:trang1234@192.168.40.77
auth_strategy = keystone
[agent]
[cors]
[database]
[keystone_authtoken]
www_authenticate_uri = http://192.168.40.78:5000
auth_url = http://192.168.40.78:5000
memcached_servers = 192.168.40.71:11211,192.168.40.76:11211,192.168.40.77:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = trang1234
[matchmaker_redis]
[nova]
[oslo_concurrency]
lock_path = /var/lib/neutron/tmp
EOF

cat <<EOF > /etc/neutron/plugins/ml2/openvswitch_agent.ini
[DEFAULT]
[agent]
tunnel_types = vxlan
l2_population = True
[network_log]
[ovs]
bridge_mappings = provider:br-provider
local_ip = 192.168.50.73
[securitygroup]
enable_security_group = True
firewall_driver = iptables_hybrid
[xenapi]
EOF

systemctl restart neutron-openvswitch-agent openstack-nova-compute openvswitch.service
```



## Tham khảo

[1] https://www.percona.com/blog/2014/09/01/galera-replication-how-to-recover-a-pxc-cluster/

[2] https://www.golinuxcloud.com/configure-haproxy-in-openstack-high-availability/

**Ghi chú**

Để có thể HA cho controller với các service như keyston, nova, neutron, glance, rabbitmq, memcache, mysql, novnc cần sử dụng pacemaker, galera, và rabbitmq cluster

+ Với pacemaker thì cần có một VIP cho cả ba server controller để HA cho keyston, nova, neutron, glance, httpd, mysql, novnc. Tạo hai resource VIP va haproxy, khi khởi động resource haproxy, cần tắt toàn bộ các service cần HA, kể cả haproxy service cũng phải tắt, sau đó thực hiện enable resource haproxy, service haproxy sẽ tự động được bật, sau đó bật các service khác. 

+ Muốn restart haproxy thì cần sử dụng pcsd chứ không nên dùng systemd

+ Khi tạo resource thì nên để VIP và Haproxy khởi chạy cùng nhau và chỉ định node cho nó chạy luôn rồi cấu hình thống nhất trên node đó

+ Cần đổi toàn bộ các endpoint cũ sang ip của VIP, trước khi xóa, thì nên tạo các endpoint của VIP trước, nhất là với keystone, tránh trường hợp xóa endpoint keystone xong không bootstap được :3 

+ Bind lại các IP của các service, bind vào ip của máy chứ không để 0.0.0.0 nữa (nova, keystone,...)

+ Galera cần khởi động một node trước, làm cho nó thành master, sau đó khởi động service trên các node còn lại, và join vào cluster.

+ RabbitMQ thì cần đặt dúng tên hostname để dùng nó join vào cluster, sau đó chọn một nó làm cluster, thực hiện join vào cluster trên hai node còn lại 
