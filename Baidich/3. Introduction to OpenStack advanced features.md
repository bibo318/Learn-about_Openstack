# Introduction to OpenStack advanced features

**DỊCH BÀI**: http://int32bit.me/2018/05/17/OpenStack%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E7%AE%80%E4%BB%8B/


Trong bài "Rare but useful operations of OpenStack", một số các tính năng hiếm khi được đề cập đến nhưng nó khá hữu dụng được giới thiệu. Bài này sẽ giới thiệu một số các tính năng nâng cao của OpenStack. Các tính năng được gọi là nâng cao ở đây đề cập đến cấu hình chung của OpenStack, không bắt buộc đối với tất cả mọi người, nhưng được thiết kế riêng cho các trường hợp cụ thể.

## 1. virtual machine soft delete

[Xem thêm](../Advance/1.%20Soft-delete-vm.md)


## 2. CPU topology and core binding

### 2.1 Tổng quan

Openstack K version giới thiệu nhiều về tính năng nâng cao CPU, không những hỗ trợ các tùy chọn CPU topology functions, mà còn hỗ trợ cài đặt virtual machine CPU socket, cores, thread,... Nó cũng hỗ trợ CPU pinning function đó là CPU core binding, và thậm chí còn có khả năng cấu hình VM độc quyền. CPU vật lý, vCPU của VM có thể được ràng buộc cố định với pCPU được chỉ định bởi host. Trong suốt quá trình chạy, CPU floatinh không xảy ra, tổng phí chuyển đổi sẽ được giảm, và tính toán hiệu năng trên VM sẽ tăng lên. Hơn nữa Openstack cũng hỗ trợ cấu hình thread policy, có thể tối ưu hóa hơn nữa hiệu năng của VM bằng việc sử dụng SMT feature của host.

Tiếp theo, tôi sẽ giới thiệu ngắn gọn cách làm thế nào để cấu hình các tính năng nâng cao của Openstack CPU.

### 2.2 Planning CPU và Memory

Trước khi cấu hình bạn cần biết plan CPU và Memory của compute node, CPU nào sẽ được cấp phát cho VM, CPU nào sẽ được để dành cho các tiến trình dành riêng cho hệ thống, và bao nhiêu memory được để riêng. Để tối ưu hóa hiệu năng, bạn cũng cần xem xét NUMA kiến trúc CPU của host.

Trong môi trường Linux, bạn có thể xem thông tin về CPU bởi command `lscpu`

```sh
$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                40
On-line CPU(s) list:   0-39
Thread(s) per core:    2
Core(s) per socket:    10
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz
Stepping:              2
CPU MHz:               1201.480
BogoMIPS:              4603.87
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              25600K
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39
```

Từ thông tin bên trên có thể cho ta thấy được host đang có tổng cộng 2 socket, mỗi socket 10 core, mỗi core mở 2 thread, vậy tổng cộng là có 40 logical CPUs, trong đó có hai NUMA node, node0 bao gồm 0,2,4,6,8,10,12,14,16,... node1 gồm 1,3,5,7,9,11,13,15,17,...

Số lượng các CPUs và memory để dành cho host cần được điều chinh theo các điều kiện thực tế. Ví dụ, nếu compute node và storage node nằm trên một node thì sẽ có nhiều CPUs được để dành hơn để đảm bảo hiệu năng của storage service.

Ví dụ này chỉ được sử dụng để test. Môi trường test để dành 4 logical CPUs (1-3) và 4GB physical memory cho host. Phần còn lại của resource sẽ được cấp phát cho VM.

CPU set (cpuset) được cấu hình bằng tùy chọn `vcpu_pin_set`, hiện tại thì đang có ba định dạng được hỗ trợ:

* `1,2,3` Specify the CPU number, separated by commas.
* `2-15, 18-31` Use - indicates continuous CPU sequences, separated by commas.
* `^0,^1,^2,^3` Use `^` the CPU number indicating the exclusion, and the rest are used as virtual machines.

Ba định dạng trên có thể được sử dụng kết hợp chung với nhau. Ví dụ trên compute node tham khảo cấu hình như sau:

```
# /etc/nova/nova.conf [DEFAULT]
...
vcpu_pin_set = ^0,^1,^2,^3
reserved_host_memory_mb = 4096
...
```

Nếu bạn cần cấu hình VM CPU exclusive, bạn cần cấu hình kernel với tùy chọn `isolcpu` để hạn chế các tiến trình khác bằng việc chỉ định CPU. Ví dụ, chúng ta cần CPU 2, 3, 6, 7 như CPU pinning exclusive cho VM, thiết lập như sau:

```sh
grubby --update-kernel=ALL --args="isolcpus=2,3,6,7"
```

Cài đặt lại grub:

```sh
grub2-install /dev/sda
```

Khởi động lại host:

	reboot

Dưới đây là tham số kernel được thêm mặc định trong lần khởi động tiếp theo của hệ thống.

	linux /vmlinuz-xxx root=xxx ... isolcpus=2,3,6,7

Trên nova-schedule node cần cấu hình filter mặc định, filter phải bao gồm `AggregateInstanceExtraSpecFilter` và ` NUMATopologyFilter` 

```sh
# /etc/nova/nova.conf [DEFAULT]
scheduler_default_filters=NUMATopologyFilter,AggregateInstanceExtraSpecsFilter,...
```

Sau đó khởi động lại dịch vụ:

	systemctl restart openstack-nova-scheduler

### 2.3 Creating a Host Collection

Trong môi trường thực tế, không phải tất cả các node computing bật các tính năng nâng cao, và đặc điểm của CPU là khác nhau. Chúng ta có thể đặt tất cả các host có cấu hình CPU giống nhau trong cùng một bộ siêu tập thông qua host aggregate, để phân biệt node compute nào enable CPU core binding function, node nào không.

Đầu tiên tạo một pinned-cpu host collection:

	nova aggregate-create pinned-cpu

Thêm metadata để phân biệt, các host trong nhóm này đã được `pinned`

	nova aggregate-set-metadata pinned-cpu pinned=true

Thêm hai host được cấu hình tính năng CPU core binding vào host collention:

	nova aggregate-add-host pinned-cpu server-1
	nova aggregate-add-host pinned-cpu server-2

Tại đây, nova scheduler nhận được metadata chứa `pinned=true` sẽ tự động gửi tới các node commpute nằm trong host aggregate `pinned-cpu`

### 2.4 Creating a flavor

Hiện tại nova không hỗ trợ metadata được chỉ định trực tiếp từ host collention (chỉ hỗ trợ chỉ định các sever group). Nó cần được cấu hình với extra specs của flavor và matched với metadata của host collection. Các host unmatched sẽ bị lược bỏ và sẽ không được chọn như một ứng cử viên để tạo VM.

Flavor hỗ trợ nhiều built-in extra specs (các thông số kỹ thuật được tích hợp thêm). Với built-in extra specs, bạn có thể cấu hình CPU topology, QoS, CPU pinning strategy, NUMA topology và PCI passthrough của VM, những ở đây sẽ chỉ nói về CPU topology và core binding. 

Để thiết lập CPU topology, tùy chọn các thông số CPU sockets, số lượng core và số lượng hyperthreads ta sử dụng lệnh như dưới đây:

```sh
$ nova flavor-key FLAVOR-NAME set \
    hw:cpu_sockets=FLAVOR-SOCKETS \
    hw:cpu_cores=FLAVOR-CORES \
    hw:cpu_threads=FLAVOR-THREADS \
    hw:cpu_max_sockets=FLAVOR-SOCKETS \
    hw:cpu_max_cores=FLAVOR-CORES \
    hw:cpu_max_threads=FLAVOR-THREADS
```

**Chú ý**: Câu lệnh trên không cần đầy đủ các tùy chọn, có thể lược bỏ bớt, chỉ để lại một vài cấu hình các option, các giá trị còn lại sẽ được tự động tính toán.

Ví dụ CPU core binding cấu hình syntax như sau:

```sh
$ nova flavor-key set FLAVOR-NAME \
    hw:cpu_policy=CPU-POLICY \
    hw:cpu_thread_policy=CPU-THREAD-POLICY
```

Các giá trị `CPU-POLICY` hợp lệ là `shared` và `dedicated`, mặc định sẽ là `share`, CPU core binding sẽ không được thực hiện. Chúng ta cần thiết lập gia trị `dedicated`. `CPU-THREAD-POLICY` liên quan đến SMT, có các gia trị hợp lệ sau:

* Prefer: The host does not necessarily need to conform to the SMT architecture. If the host has an SMT architecture, thread siblings will be assigned first.
* Isolate: The host SMT architecture is not required. If the host does not have the SMT architecture, each vCPU will be bound to a different pCPU. If the host is an SMT architecture, each vCPU is bound to a different physical core.
* Require: The host must satisfy the SMT architecture. Each vCPU is allocated on a different thread siblins. If the host does not have the SMT architecture or the core idle thread siblings does not satisfy the requested number of vCPUs, the scheduling will fail

Thông thường giá trị mặc định là `prefer` hoặc `isolate`

Tiếp theo, bắt đầu tạo flavor, set to 8CPUs, 2GB RAM, 20GB disk:

	nova flavor-create m1.xlarge.pinned 100 2048 20 8

Set the CPU policy

	nova flavor-key m1.xlarge.pinned set hw:cpu_policy=dedicated

Thêm  extra specs liên quan tới pinned để match với host collention metadata, và đảm bảo chỉ có host với core binding enable mới được lựa chọn trong khi scheduling:

	nova flavor-key m1.xlarge.pinned set aggregate_instance_extra_specs:pinned=true

Cấu hình CPU topology với 2 sockets * 2 cores * 2 threads

```sh
nova flavor-key m1.xlarge.pinned set \
    hw:cpu_sockets=2 \
    hw:cpu_cores=2 \
    hw:cpu_threads=2
```

View the extra specs information for the flavor:

	nova flavor-show m1.xlarge.pinned | awk -F '|' '/extra_specs/{print $3}' | python -m json.tool { "aggregate_instance_extra_specs:pinned": "true", "hw:cpu_cores": "2", "hw:cpu_policy": "dedicated", "hw:cpu_sockets": "2", "hw:cpu_threads": "2" }

### 2.5 Functional Verification

Tạo một VM với flavor mới được tạo:

```sh
nova boot  int32bit-test-pinning \
	--flavor m1.xlarge.pinned  \
	--image 16b79884-77f2-44f5-a6d7-6fcc30651283\
	--nic net-id=ed88dc5a-61d8-4f99-9532-8c68e5ec5b9e
```

Sử dụng `nova show` để xem thông tin về host mới tạo, xem nó nằm trên host compute nào, sau đó xem file xml của VM:

	virsh dumpxml <uuid-vm>

Trong file xml bạn có thể thấy:

```sh
<vcpu placement='static'>8</vcpu>
<cputune>
<vcpupin vcpu='0' cpuset='25'/>
<vcpupin vcpu='1' cpuset='5'/>
<vcpupin vcpu='2' cpuset='8'/>
<vcpupin vcpu='3' cpuset='28'/>
<vcpupin vcpu='4' cpuset='9'/>
<vcpupin vcpu='5' cpuset='29'/>
<vcpupin vcpu='6' cpuset='24'/>
<vcpupin vcpu='7' cpuset='4'/>
<emulatorpin cpuset='4-5,8-9,24-25,28-29'/>
</cputune>
```

Đây là binding relationship between vCPU and pCPU.

Kết quả có thể xem CPU information trong VM 

```sh
# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                2
On-line CPU(s) list:   0,1
Thread(s) per core:    2
Core(s) per socket:    2
Socket(s):             2
NUMA node(s):          1
Vendor ID:             GenuineIntel
...
```

Ta có thể thấy các giá trị giống với khi cấu hình (2 sockets * 2 cores * 2 threads)

## 3. Research CPU topology 

### Processor affinity 

Processor affinity hay CPU pinning kích hoạt binding hoặc unbinding của process hoặc một thread trên một CPU hoặc một dải các CPUs. Process hoặc thread sẽ chỉ thực hiện trên một hoặc nhiều CPU được chỉ định. 

### CPU topologies

Tính năng NUMA topology và CPU pinning trong OPS cung cấp các high-level control với các instance chạy trên hypervisor CPUs và topology của virtual CPUs có sẵn cho instance. Các tính năng này giúp tối thiểu hóa độ trê và tối đa hóa hiệu suất.

#### SMP, NUMA, and SMT

* **Symmetric multiprocessing (SMP)**: SMP là một thiết kế được tìm thấy trong hệ thống multi-core hiện đại. Một hệ thống SMP có hai hoặc nhiều CPUs và các CPUs này được kết nối bởi một vài kết nối. Điều này cung cấp cho CPUs quyền truy cập như nhau vào các nguyền tài nguyên của hệ thống như memory và các input/output port.

* **Non-uniform memory access (NUMA)**: NUMA là một dạng khác của thiết kế SMP được tìm thấy trong nhiều hệ thống multi-socket. Trong hệ thống NUMA, system memory được chia trong các cells hoặc các nodes liên kết với các CPUs cụ thể. Yêu cầu memory trên các nodes khác nhau có thể thông qua một interconnect bus. Tuy nhiên, băng thông chia sẻ các bus được hạn chế. Do đó việc này có thể gây ra xung đột và ảnh hưởng tới hiệu suất.

* **Simultaneous Multi-Threading (SMT)**: SMT là thiết kế bổ sung cho SMP. Trong khi CPUs trong hệ thống SMP share một bus và một vài memmory, thì CPUs trong hệ thống SMT share nhiều hơn các components. Các thành phần của CPU đưuọc biết đến như thread siblings. Tất cả các CPUs xuất hiện nư các usable CPUs trên hệ thống và có thể thực hiện workloads song song. Tuy nhiên với NUMA cạnh trang giữa các nguồn tài nguyên được chia sẽ giữa các thread.


Trong OpenStack, SMP CPUs được biết đến như `cores`, NUMA cells hoặc nodes được biết đến như các `socket`, và SMT CPUs được biết đến như các `threads`. Ví dụ, ta có một `quad-socket`, 8 core với Hyper-Threading sẽ có 4 socket, 8 core trên một socket, 2 thread trên một core, vậy tổng số CPU là 64 CPUs.

#### Customizing instance CPU pinning policies

Mặc định thì, các instance vCPU process không được gán cho bất kỳ một CPU nào của máy, thay vào đó chúng sẽ float cross host CPUs giống như bất kỳ tiến trình nào khác. Điều này cho phép nhiều tính năng khác giống như overcommiting của CPUs. Trong hệ thống nặng, điều này cung cấp tối ưu hóa hiệu năng hệ thống và độ trễ cho từng instance riêng biệt.

Một vài workloads yêu cầu real-time hoặc gần như real-time behavior, cái mà không thể thực hiện với độ trễ được đưa ra bởi CPU policy mặc định. Đối với workload như vậy, nó có lợi cho việc điều khiển các host CPUs bị ràng buộc với vCPU của một instance. Tiến trình này được gọi là pinning. Không có instance với pinned CPUs có thể sử dụng các CPUs của các pinned instance khác. Vì vậy có thể ngăn chặn được việc xung đột tài nguyên giữa các instances. Để cấu hình 1 flavor sử dụng pinned vCPUs ta sử dụng một dedicated CPU policy. 

	openstack flavor set m1.large --property hw:cpu_policy=dedicated

> Chú ý: Host aggregates nên được sử dụng để phân biệt các pinned instance với các unpinned instances 

Khi chạy workloads trên SMT hosts, điều quan trọng là nó phải nhận ra được tác động của thread siblings có thể có. Thread siblings chia sẻ các thành phần và cạnh tranh giữa các thành phàn có thể ảnh hường tới hiệu năng. Để cấu hình sử dụng thread, một CPU thread policy nên được chỉ định. Đối với workloads, nơi chia sẽ lợi ích của hiệu năng, sử dụng thread siblings. Để thực hiện chạy lệnh sau:

```sh
$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=require
```

Với các workloads khác, nơi mà hiệu năng bị ảnh hưởng bởi sự tranh nhau tài nguyên hệ thống, sử dụng non-thread siblings hoặc non-SMT hosts. Thực hiện như sau:

```sh
$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=isolate
```

Cuối cùng, với các workloads nơi hiệu năng ít bị ảnh hương nhất có thể sử dụng thread siblings nếu có thể. Cái này là mặc định, những cũng có thể được thiết lập như sau:

```sh
$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=prefer
```

Hiện nay các ứng dụng thường được đóng gói dươi dạng các image. Với các ứng dụng yêu cầu thời gian thực hoặc các hành vi cần thời gian thực, cấu hình image metadata để đảm bảo luôn tạo các instance được pinned với bất cứ flavor nào. Để cấu hình một image sử dụng pinned vCPUs và advoid thread siblings, chạy:

```sh
$ openstack image set [IMAGE_ID] \
  --property hw_cpu_policy=dedicated \
  --property hw_cpu_thread_policy=isolate
```

Nếu flavor được chỉ định là một CPU policy `hw_cpu_policy=dedicated` thì policy sẽ được sử dụng. Nếu flavor được chỉ định CPU policy là `shared` và image không có policy hoặc một policy `shared` thì `shared` policy sẽ được sử dụng, nhưng nếu image chỉ định là một policy `dedicated` một trường hợp ngoại lệ. Thiết lập một `shared` policy thông qua flavor extra-specs, người quản trị có thể ngăn cản các users cấu hình CPU policies trong images và ảnh hưởng tới việc tần dụng các tài nguyên. Để cấu hình policy này, chạy command:

	$ openstack flavor set m1.large --property hw:cpu_policy=shared

Nếu các flavor không chỉ rõ CPU thread policy thì CPU thread policy được chỉ định bởi image (nếu có) sẽ được sử dụng. Nếu cả flavor và image đều chỉ định một CPU thread policy thì chúng sẽ phải cùng chỉ định same policy, nếu không sẽ lại một `exception will be raised` 


#### Customizing instance CPU topologies

Ngoài việc cấu hình một instance được scheduled trên host CPUs, nó còn có thể cấu hình cách mà CPU thể hiện trong instance của chính nó. Mặc định, khi instance NUMA placement không được chỉ định, một topology với N sockets, mỗi cái có một core và một thread, và được sử dụng cho một instance, N ở đây sẽ tương ứng với số lượng vCPU của instance được yêu cầu. Khi instance NUMA placement được chỉ định, số lượng của sockets sẽ được fixed với số lượng NUMA nodes được sử dụng và tổng số lượng CPUs instance sẽ được chia ra các socket.

Một vài workloads được hưởng lợi từ một custom topology. Ví dụ, một hệ thống, các lincence khác nhau có thể cần phụ thuộc vài số lượng của CPU sockets. Để cấu hình một flavor sử dụng tối đa hai sockets, chạy:

	$ openstack flavor set m1.large --property hw:cpu_sockets=2

Tương tự, để cấu hình một flavor sử dụng một core một thread chạy command sau:

```sh
$ openstack flavor set m1.large \
  --property hw:cpu_cores=1 \
  --property hw:cpu_threads=1
```

Ngoài ra nó có thể set một giới hạn trên cho số lượng socket, cores và thread được sử dụng. Không giống với các giá trị cứng bên trên, các gia trị bên trên cần độ chính xác, và để ý tới số lượng có từng thông số, nhưng với thuộc tính này thì không cần, bởi nó chỉ cung cấp một giới hạn không được phép vượt quá. Điều này thường được sử dụng cho một vài việc đặt lịch linh động


[Xem thêm](https://docs.openstack.org/nova/pike/admin/cpu-topologies.html)




## 4. Virtualization nesting

### 4.1 Turn on virtualization nesting

Mặc định, tính năng CPU của KVM được tạo không bao gồm vmx, điều đó nghĩa là các VMs sử dụng bộ tăng tốc KVM hardware sẽ không thể tạo VM, cho thấy virtualization nesting không được hỗ trợ. May mắn là ngoài VirttualBox, một số các hypervisor chính như VMware, KVM, Xen cũng hỗ trợ virtualized Nested. Nói cách khác, chúng ta có thể tạo một VM cũng hỗ trợ KVM trên một host hỗ trợ KVM

Lấy KVM như một ví dụ, đầu tiên, kiểm tra KVM nesting function đã được kích hoạt trên hệ thống hay chưa:

	cat /sys/module/kvm_intel/parameters/nested 

Nếu output là `N`  thì KVM nesting function không được hỗ trợ. Bạn có thể kích hoạt nó bằng cách chỉnh sửa cấu hình trong file `/etc/modprobe.d/kvm-intel.conf` như sau:

	options kvm-intel nested=1

Reload kvm-intel kernel module:

	rmmod kvm_intel
	modprobe kvm_intel

### 4.2 Cấu hình compute node

Openstack hỗ trợ virtualization nesting, chỉnh sửa file cấu hình của nova trên compute node `/etc/nova/nova.conf`, thiết lập tùy chọn `cpu_mode = host-passthrough`, sau đó khởi động lại nova-compute service.

Chú ý, khi migration function được sử dụng cho VM `host-passthrough` thì nó bị giới hạn và chỉ migrate lên các host có cấu hình tương tự.



## Tham khảo

[1] https://docs.openstack.org/nova/pike/admin/flavors.html

